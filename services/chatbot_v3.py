# ì¶”ê°€: ì½”ë“œ ë§¨ ìœ„ì— clean_text_for_matching í•¨ìˆ˜ ì •ì˜
#!/usr/bin/env python3
# chatbot.py  Â·  Adaptive Filtering + KeywordÂ·Category Edition
# ì‹¤í–‰: python3 chatbot.py
# í•„ìš”í•œ íŒ¨í‚¤ì§€: pip install langchain-openai langchain chromadb python-dotenv

import os, re, json
from typing import List, Tuple, Optional
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import Chroma
from langchain.schema import Document
from langchain.prompts import (
    ChatPromptTemplate, 
    SystemMessagePromptTemplate, 
    HumanMessagePromptTemplate
)
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.text_splitter import RecursiveCharacterTextSplitter
from tqdm import tqdm
from functools import lru_cache
import time

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
# ğŸ“Œ Inâ€‘memory session store (userâ€‘level)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
from collections import defaultdict
SESSION_STORE = defaultdict(lambda: {"user_info": None, "recommended_ids": set()})

# íŠ¹ìˆ˜ë¬¸ì ì œê±°, ì†Œë¬¸ìí™” ë“±ì„ í†µí•´ í‚¤ì›Œë“œ ë§¤ì¹­ì— ë°©í•´ê°€ ë˜ëŠ” ìš”ì†Œë“¤ì„ ì œê±°í•˜ëŠ” í•¨ìˆ˜
def clean_text_for_matching(text):
    return re.sub(r"[^\w\s]", "", text).replace("ì—", "").replace("ì—ì„œ", "").replace("ì¸ë°", "").replace("ì•¼", "").strip()
# ì‚¬ìš©ìê°€ â€œë‹¤ë¥¸ ì •ì±…â€, â€œì¶”ê°€ë¡œ ë³´ì—¬ì¤˜â€ ê°™ì€ ì¶”ê°€ ì¶”ì²œ ìš”ì²­ì¸ì§€ íŒë³„í•˜ëŠ” í•¨ìˆ˜
def is_generic_more_request(text: str) -> bool:
    """
    ì‚¬ìš©ìê°€ 'ë‹¤ë¥¸ ì •ì±…', 'ì¶”ê°€ ì •ì±…', 'ë” ë³´ì—¬ì¤˜' ë“±
    êµ¬ì²´ì  ì¡°ê±´ ì—†ëŠ” ì¶”ê°€ ì¶”ì²œì„ ìš”êµ¬í•˜ëŠ”ì§€ íŒë³„.
    """
    text = text.strip()
    if re.search(r"ë‹¤ë¥¸\s*ì •ì±…", text):
        return True
    if "ì •ì±…" in text and re.search(r"(ë”|ì¶”ê°€|ë˜|ì—†ì–´|ìˆì–´)", text):
        return True
    # "ë” ì•Œë ¤ì¤˜", "ë” ë³´ì—¬ì¤˜", "ë” ì¶”ì²œí•´ì¤˜" ì²˜ë¦¬
    if re.search(r"ë”\s*(ì•Œë ¤ì¤˜|ë³´ì—¬ì¤˜|ì¶”ì²œí•´ì¤˜)", text):
        return True
    return False

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
# ì •ì±… ê´€ë ¨ ì§ˆë¬¸ ì—¬ë¶€ íŒë³„ í•¨ìˆ˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
NON_POLICY_KEYWORDS = [
    "ì•ˆë…•", "í•˜ì´", "ë°˜ê°€ì›Œ", "ì˜ ì§€ëƒˆì–´", "ë­í•´", "ì‹¬ì‹¬í•´", "ë†€ì", "ì§€ë£¨í•´", "ê³ ë§ˆì›Œ", "ê°ì‚¬", "ì˜ì", "ì˜ ì", "êµ¿ë°¤",
    "ëˆ„êµ¬ì•¼", "ë„ˆ ë­ì•¼", "ì •ì²´ê°€ ë­ì•¼", "ìê¸°ì†Œê°œ", "ì´ë¦„", "ì±—ì§€í”¼í‹°", "gpt", "aiì•¼", "ë¡œë´‡ì´ì•¼", "ëª‡ ì‚´", "ë‚˜ì´",
    "ë‚ ì”¨", "ì˜¨ë„", "ê¸°ì˜¨", "ëª‡ ì‹œ", "ì‹œê°„", "ì˜¤ëŠ˜ ë‚ ì§œ", "ì§€ê¸ˆ ëª‡ì‹œ", "ì˜¤ëŠ˜ ë­ì•¼", "ìš”ì¼",
    "ê¸°ë¶„ ì–´ë•Œ", "ì‚¬ë‘í•´", "ê·€ì—¬ì›Œ", "ì¢‹ì•„í•´", "ì—¬ìì¹œêµ¬", "ë‚¨ìì¹œêµ¬", "ì¸", "ì—°ì• ", "ì´ìƒí˜•",
    "í€´ì¦ˆ", "ìˆ˜ìˆ˜ê»˜ë¼", "ë†ë‹´", "ì›ƒê²¨ì¤˜", "ì¬ë°ŒëŠ” ì–˜ê¸°", "ìš°ì£¼", "ê³¼í•™", "ì—­ì‚¬", "ìœ íŠœë¸Œ", "ê²Œì„", "ìœ ë¨¸"
]
# ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±ê³¼ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸(NON_POLICY_KEYWORDS)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ, ì…ë ¥ì´ ì •ì±… ê´€ë ¨ ì§ˆì˜ì¸ì§€ 1ì°¨ ê²€ì‚¬í•˜ëŠ” í•¨ìˆ˜
def is_policy_related_question(text: str) -> bool:
    import re
    # uses global clean_text_for_matching and REVERSE_REGION_LOOKUP
    if len(text.strip()) < 2:
        return False
    cleaned = re.sub(r"[ã…‹ã…ã… ã…œ]+", "", text.lower())
    for word in NON_POLICY_KEYWORDS:
        if word in cleaned:
            return False
    if re.match(r"^[ê°€-í£]{1,3}(ì•¼|ì´ì•¼)?$", text.strip()):
        # Clean conversational endings like 'ì•¼', 'ì´ì•¼'
        clean_key = clean_text_for_matching(text)
        # If cleaned key matches a region in lookup, treat as policy-related (region input)
        if clean_key in REVERSE_REGION_LOOKUP:
            return True
        return False
    return True

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
# LLM ê¸°ë°˜ ì •ì±… ì§ˆë¬¸ ì—¬ë¶€ íŒë³„ í•¨ìˆ˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
# ìœ„ íœ´ë¦¬ìŠ¤í‹±ì´ ëª¨í˜¸í•  ë•Œ, GPT-4o-miniì— â€œY/Nâ€ ë¶„ë¥˜ë¥¼ ìš”ì²­í•´ ë³´ë‹¤ ì •í™•íˆ íŒë‹¨í•˜ê³ , ì‹¤íŒ¨ ì‹œ rule-basedë¡œ í´ë°±
@lru_cache(maxsize=1024)           # ê°™ì€ ë¬¸ì¥ì€ í•œ ë²ˆë§Œ ë¬¸ì˜
def is_policy_related_question_llm(text: str) -> bool:
    """
    GPT-4o-minië¡œ â€˜ì •ì±… ê´€ë ¨ ì§ˆë¬¸ì¸ì§€â€™ Y/N ë¶„ë¥˜.
    - refined heuristic for short/numeric/keyword input
    - LLM ì˜¤ë¥˜ ì‹œ rule-based í´ë°±.
    """
    cleaned = text.strip()
    if not cleaned:
        return False  # ë¹ˆ ì…ë ¥

    # 'ë‹¤ë¥¸ ì •ì±…', 'ì¶”ê°€ ì •ì±…' ë“± ì¼ë°˜ ì¶”ê°€ ì¶”ì²œ ìš”ì²­ì€ ì •ì±… ê´€ë ¨ìœ¼ë¡œ ê°„ì£¼
    if is_generic_more_request(cleaned):
        return True

    # â‘  ìˆ«ì 1~2ìë¦¬ë§Œ ì…ë ¥ â†’ ë‚˜ì´ë¡œ ê°„ì£¼ â†’ ì •ì±… ì§ˆë¬¸ True
    if re.fullmatch(r"\d{1,2}", cleaned):
        return True

    # â‘¡ ë‹¨ì–´ 1~2ìì—¬ë„ ê´€ì‹¬ì‚¬Â·ì§€ì—­ í‚¤ì›Œë“œë¼ë©´ True
    if cleaned in REVERSE_REGION_LOOKUP:
        return True
    if cleaned in INTEREST_MAPPING:
        return True
    if any(cleaned in kws for kws in INTEREST_MAPPING.values()):
        return True

    # â‘¢ 1ê¸€ìÂ·íŠ¹ìˆ˜ë¬¸ìÂ·ì›ƒìŒ(ã…‹ã…)ë§Œ â†’ False
    if len(cleaned) == 1 or re.fullmatch(r"[ã…‹ã…]+", cleaned):
        return False

    system_msg = (
        "ë„ˆëŠ” ëŒ€í•œë¯¼êµ­ ì²­ë…„ ì •ì±… ìƒë‹´ ì±—ë´‡ì˜ ë¶„ë¥˜ê¸°ì•¼. "
        "ì•„ë˜ ì‚¬ìš©ì ì…ë ¥ì´ ì •ì±…ê³¼ *ê´€ë ¨ëœ ì§ˆë¬¸*ì¸ì§€ íŒë‹¨í•´. "
        "ëŒ€ë‹µì€ 'Y' ë˜ëŠ” 'N' ì¤‘ í•˜ë‚˜ë¡œë§Œ."
    )
    user_msg = f"ì‚¬ìš©ì ì…ë ¥: {cleaned}\n\nì •ì±… ê´€ë ¨ ì§ˆë¬¸ì¸ê°€?"

    try:
        resp = openai.ChatCompletion.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_msg},
                {"role": "user",   "content": user_msg},
            ],
            temperature=0,
            max_tokens=1,
        )
        return resp.choices[0].message.content.strip().upper().startswith("Y")
    except Exception:
        # ë„¤íŠ¸ì›Œí¬/ì¿¼í„° ë¬¸ì œ ì‹œ íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ í´ë°±
        return is_policy_related_question(cleaned)

# í† í° ìˆ˜ë‚˜ í•µì‹¬ í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€ë¥¼ ë³´ê³  â€œìœ íš¨í•œ ì •ì±… ì§ˆì˜â€ì¸ì§€ ì¶”ê°€ ê²€ì‚¬í•˜ëŠ” í•¨ìˆ˜
def is_valid_query(text: str) -> bool:
    # ìˆ«ìë§Œ ì…ë ¥ë˜ì–´ë„ ë‚˜ì´ë¡œ ê°„ì£¼
    if re.search(r"\b\d{1,2}\b", text):
        return True

    tokens = re.findall(r"[ê°€-í£a-zA-Z0-9]+", text)

    # 1) 3ë‹¨ì–´ ì´ìƒì´ë©´ ë¬´ì¡°ê±´ ì •ì±… ê´€ë ¨ ì§ˆì˜ë¡œ ê°„ì£¼
    if len(tokens) >= 3:
        return True

    # 2) 2ë‹¨ì–´ì§œë¦¬ ì§§ì€ ì§ˆë¬¸ì´ë¼ë„ í•µì‹¬ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ë©´ í—ˆìš©
    if len(tokens) == 2:
        for tok in tokens:
            if (
                tok == "ì •ì±…" or
                tok in KEYWORDS or
                tok in INTEREST_MAPPING or
                any(tok in kws for kws in INTEREST_MAPPING.values())
            ):
                return True
    return False
# ì§€ì—­ëª… í‚¤ì›Œë“œ ì—¬ë¶€ íŒë³„ í•¨ìˆ˜
def is_region_keyword(word: str) -> bool:
    return word in REVERSE_REGION_LOOKUP or any(word in names for names in REGION_MAPPING.values())

# ì‚¬ìš©ì ì…ë ¥ì—ì„œ ì •ë³´ ìë™ ì¶”ì¶œ í•¨ìˆ˜
def extract_user_info(user_input: str):
    info = {"age": None, "region": None, "interests": [], "status": None, "income": None}

    # âœ… ì „ì²˜ë¦¬: ë§ˆì¹¨í‘œ, ì‰¼í‘œ ë“± ì œê±° â†’ 'ì—¬ì£¼ì— ì‚¬ëŠ” 25ì‚´ì´ì•¼'ë¡œ ë§Œë“¤ê¸°
    clean_text = re.sub(r"[^\wê°€-í£]", " ", user_input)
    clean_text = re.sub(r"\s+", " ", clean_text).strip()

    # ğŸ” ì •í™•í•œ ë‚˜ì´/ì§€ì—­/ê´€ì‹¬ì‚¬ íŒŒì‹±ì€ parse_user_input() ì¬í™œìš©
    parsed_age, parsed_region, parsed_interests = parse_user_input(clean_text)
    info["age"] = parsed_age
    info["region"] = parsed_region
    info["interests"] = parsed_interests if parsed_interests else []

    # ìƒíƒœ ì¶”ì¶œ
    if "ëŒ€í•™ìƒ" in user_input:
        info["status"] = "ëŒ€í•™ìƒ"
    elif "ì·¨ì¤€ìƒ" in user_input or "ì·¨ì—… ì¤€ë¹„" in user_input:
        info["status"] = "ì·¨ì—…ì¤€ë¹„ìƒ"

    # ì†Œë“
    if "ì €ì†Œë“" in user_input:
        info["income"] = "ì €ì†Œë“ì¸µ"
    elif "ê³ ì†Œë“" in user_input:
        info["income"] = "ê³ ì†Œë“ì¸µ"

    return info


def print_result(idx, doc):
    result = {
        "policy_id": doc.metadata.get("policy_id", f"unknown_{idx}"),
        "name":      doc.metadata.get("title"),
        "summary":   doc.metadata.get("summary"),
        "eligibility": f"{doc.metadata.get('min_age','?')}~{doc.metadata.get('max_age','?')}ì„¸ / {doc.metadata.get('region','ì „êµ­')}",
        "period":    doc.metadata.get("apply_period",""),
        # â†“ ë””ë²„ê¹…
        # "score":     doc.metadata.get("debug_total_score"),
        # "region":    doc.metadata.get("debug_region_score"),
        # "interest":  doc.metadata.get("debug_interest_score"),
        # "keyword":   doc.metadata.get("debug_keyword_score")
    }
    print(json.dumps(result, ensure_ascii=False, indent=2))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
# ê¸€ë¡œë²Œ ì„ë² ë”© ë° í‚¤ì›Œë“œ ë²¡í„°DB (í‚¤ì›Œë“œ ì „ìš©)
# Load embedding function globally
from dotenv import load_dotenv
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY", "")
import openai
openai.api_key = api_key
embedding = OpenAIEmbeddings()
# Load keyword vectorstore (ensure it's built with keyword terms only)
keyword_vectordb = Chroma(persist_directory="./kwdb", embedding_function=embedding)
category_vectordb = Chroma(persist_directory="./categorydb", embedding_function=embedding)
# Main policy vectorstore
policy_vectordb = Chroma(persist_directory="./chroma_policies", embedding_function=embedding)
# 0. ë³´ì¡° í•¨ìˆ˜ â€“ ì§ˆì˜ ì¬êµ¬ì„±
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #

def build_query(base_prompt: str,
                age: Optional[int],
                region: Optional[str],
                interests: Optional[List[str]]) -> str:
    """ì €ì¥ëœ ì •ë³´ë¥¼ ì—®ì–´ RAGìš© ìì—°ì–´ ì§ˆì˜ ë¬¸ìì—´ ìƒì„±"""
    parts: List[str] = [base_prompt]
    if region:
        parts.append(f"{region} ê±°ì£¼")
    if age:
        parts.append(f"{age}ì„¸")
    if interests:
        parts.append(f"ê´€ì‹¬ì‚¬ {', '.join(interests)}")
    return " ".join(parts)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
# 1. ê´€ì‹¬ì‚¬ Â· ì§€ì—­ ë§µ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
INTEREST_MAPPING = {
    "ì°½ì—…": ["ì°½ì—…", "ìŠ¤íƒ€íŠ¸ì—…", "ê¸°ì—… ì„¤ë¦½", "ë²¤ì²˜", "ì†Œìƒê³µì¸", "ì‚¬ì—…", "ìê¸ˆì§€ì›"],
    "ì·¨ì—…": ["ì·¨ì—…", "ì¼ìë¦¬", "ì±„ìš©", "ê³ ìš©", "ì¡í˜ì–´", "êµ¬ì§í™œë™", "ë©´ì ‘", "ì´ë ¥ì„œ", "ìê¸°ì†Œê°œì„œ", "ì·¨ì—…ì§€ì›", "êµ¬ì§"],
    "ìš´ë™": ["ìš´ë™", "ìŠ¤í¬ì¸ ", "ì²´ìœ¡", "í”¼íŠ¸ë‹ˆìŠ¤", "í—¬ìŠ¤", "í—¬ìŠ¤ì¼€ì–´", "ìš”ê°€", "ì²´ìœ¡ê´€"],
    "í•™ì—…": ["í•™ì—…", "í•™ìŠµ", "ê³µë¶€", "êµìœ¡", "í•™ìœ„", "ëŒ€í•™ìƒí™œ", "ëŒ€í•™", "ì—°êµ¬"],
    "í”„ë¡œê·¸ë¨": ["í”„ë¡œê·¸ë¨", "ì›Œí¬ìˆ", "ì„¸ë¯¸ë‚˜", "ìº í”„", "ì—°ìˆ˜", "êµìœ¡í”„ë¡œê·¸ë¨", "í›ˆë ¨í”„ë¡œê·¸ë¨"],
    "ì¥í•™ê¸ˆ": ["ì¥í•™ê¸ˆ", "í•™ë¹„ ì§€ì›", "ë“±ë¡ê¸ˆ ì§€ì›", "êµìœ¡ë¹„ ì§€ì›", "í•™ìê¸ˆ"],
    "í•´ì™¸ì—°ìˆ˜": ["í•´ì™¸ì—°ìˆ˜", "ê¸€ë¡œë²Œ ì—°ìˆ˜", "êµí™˜í•™ìƒ", "ì–´í•™ì—°ìˆ˜", "í•´ì™¸êµìœ¡"],
    "ì¸í„´ì‹­": ["ì¸í„´ì‹­", "í˜„ì¥ì‹¤ìŠµ", "ì‚°í•™í˜‘ë ¥", "ì¸í„´", "ì‹¤ë¬´ê²½í—˜"],
    "ì£¼ê±°": ["ì£¼ê±°", "ì£¼íƒ", "ì„ëŒ€", "ì „ì„¸", "ì›”ì„¸", "ë³´ì¦ê¸ˆ", "ë¶€ë™ì‚°"],
    "ë³µì§€": ["ë³µì§€", "ì‚¬íšŒë³µì§€", "ì§€ì›", "ë³´ì¡°ê¸ˆ", "ë°”ìš°ì²˜", "ì˜ë£Œ", "ê±´ê°•", "ì¶œì‚°", "ìœ¡ì•„"],
    "ì°¸ì—¬": ["ì°¸ì—¬", "ê¶Œë¦¬", "ì‹œë¯¼", "ì‚¬íšŒ", "ë´‰ì‚¬", "í™œë™", "ë™ì•„ë¦¬"],
    "ì§ì—…êµìœ¡": ["ì§ì—…", "í›ˆë ¨", "ê¸°ìˆ ", "ìê²©ì¦", "êµìœ¡", "ê°•ì¢Œ", "ì§ì—…í›ˆë ¨"],
    "í•´ì™¸ì·¨ì—…": ["í•´ì™¸ì·¨ì—…", "êµ­ì™¸ì·¨ì—…", "ê¸€ë¡œë²Œì·¨ì—…", "ì¼ìë¦¬", "ì§„ì¶œ"],
    "ì •ì‹ ê±´ê°•": ["ì •ì‹ ê±´ê°•", "ìƒë‹´", "ì‹¬ë¦¬", "ìŠ¤íŠ¸ë ˆìŠ¤", "ìš°ìš¸ì¦"],
    "ê¸ˆìœµì§€ì›": ["ëŒ€ì¶œ", "ìê¸ˆ", "ì§€ì›ê¸ˆ", "ë³´ì¡°ê¸ˆ", "ìœµì"]
}
REGION_KEYWORDS = {
    "ì„œìš¸": ["ì„œìš¸", "ì„œìš¸ì‹œ"],
    "ê²½ê¸°": ["ê²½ê¸°", "ê²½ê¸°ë„"],
    "ì¸ì²œ": ["ì¸ì²œ", "ì¸ì²œì‹œ"],
    "ë¶€ì‚°": ["ë¶€ì‚°"],
    "ëŒ€êµ¬": ["ëŒ€êµ¬"],
    "ê´‘ì£¼": ["ê´‘ì£¼"],
    "ëŒ€ì „": ["ëŒ€ì „"],
    "ìš¸ì‚°": ["ìš¸ì‚°"],
    "ì„¸ì¢…": ["ì„¸ì¢…"],
    "ê°•ì›": ["ê°•ì›", "ê°•ì›ë„", "ê°•ì›íŠ¹ë³„ìì¹˜ë„"],
    "ì¶©ë¶": ["ì¶©ë¶", "ì¶©ì²­ë¶ë„"],
    "ì¶©ë‚¨": ["ì¶©ë‚¨", "ì¶©ì²­ë‚¨ë„"],
    "ì „ë¶": ["ì „ë¶", "ì „ë¼ë¶ë„"],
    "ì „ë‚¨": ["ì „ë‚¨", "ì „ë¼ë‚¨ë„"],
    "ê²½ë¶": ["ê²½ë¶", "ê²½ìƒë¶ë„"],
    "ê²½ë‚¨": ["ê²½ë‚¨", "ê²½ìƒë‚¨ë„"],
    "ì œì£¼": ["ì œì£¼", "ì œì£¼ë„", "ì œì£¼íŠ¹ë³„ìì¹˜ë„"]
}

REGION_MAPPING = {
    "ì„œìš¸": [
        "ì„œìš¸íŠ¹ë³„ì‹œ ì¢…ë¡œêµ¬", "ì„œìš¸íŠ¹ë³„ì‹œ ì¤‘êµ¬", "ì„œìš¸íŠ¹ë³„ì‹œ ìš©ì‚°êµ¬", "ì„œìš¸íŠ¹ë³„ì‹œ ì„±ë™êµ¬",
        "ì„œìš¸íŠ¹ë³„ì‹œ ê´‘ì§„êµ¬", "ì„œìš¸íŠ¹ë³„ì‹œ ë™ëŒ€ë¬¸êµ¬", "ì„œìš¸íŠ¹ë³„ì‹œ ì¤‘ë‘êµ¬", "ì„œìš¸íŠ¹ë³„ì‹œ ì„±ë¶êµ¬",
        "ì„œìš¸íŠ¹ë³„ì‹œ ê°•ë¶êµ¬", "ì„œìš¸íŠ¹ë³„ì‹œ ë„ë´‰êµ¬", "ì„œìš¸íŠ¹ë³„ì‹œ ë…¸ì›êµ¬", "ì„œìš¸íŠ¹ë³„ì‹œ ì€í‰êµ¬",
        "ì„œìš¸íŠ¹ë³„ì‹œ ì„œëŒ€ë¬¸êµ¬", "ì„œìš¸íŠ¹ë³„ì‹œ ë§ˆí¬êµ¬", "ì„œìš¸íŠ¹ë³„ì‹œ ì–‘ì²œêµ¬", "ì„œìš¸íŠ¹ë³„ì‹œ ê°•ì„œêµ¬",
        "ì„œìš¸íŠ¹ë³„ì‹œ êµ¬ë¡œêµ¬", "ì„œìš¸íŠ¹ë³„ì‹œ ê¸ˆì²œêµ¬", "ì„œìš¸íŠ¹ë³„ì‹œ ì˜ë“±í¬êµ¬", "ì„œìš¸íŠ¹ë³„ì‹œ ë™ì‘êµ¬",
        "ì„œìš¸íŠ¹ë³„ì‹œ ê´€ì•…êµ¬", "ì„œìš¸íŠ¹ë³„ì‹œ ì„œì´ˆêµ¬", "ì„œìš¸íŠ¹ë³„ì‹œ ê°•ë‚¨êµ¬", "ì„œìš¸íŠ¹ë³„ì‹œ ì†¡íŒŒêµ¬",
        "ì„œìš¸íŠ¹ë³„ì‹œ ê°•ë™êµ¬",
        "ì„œìš¸",
        "ì„œìš¸íŠ¹ë³„ì‹œ",
        "ì„œìš¸ì‹œ"
    ],
    "ê²½ê¸°": [
        "ê²½ê¸°ë„ ìˆ˜ì›ì‹œì¥ì•ˆêµ¬", "ê²½ê¸°ë„ ìˆ˜ì›ì‹œê¶Œì„ êµ¬", "ê²½ê¸°ë„ ìˆ˜ì›ì‹œíŒ”ë‹¬êµ¬", "ê²½ê¸°ë„ ìˆ˜ì›ì‹œì˜í†µêµ¬",
        "ê²½ê¸°ë„ ì„±ë‚¨ì‹œìˆ˜ì •êµ¬", "ê²½ê¸°ë„ ì„±ë‚¨ì‹œì¤‘ì›êµ¬", "ê²½ê¸°ë„ ì„±ë‚¨ì‹œë¶„ë‹¹êµ¬", "ê²½ê¸°ë„ ì˜ì •ë¶€ì‹œ",
        "ê²½ê¸°ë„ ì•ˆì–‘ì‹œë§Œì•ˆêµ¬", "ê²½ê¸°ë„ ì•ˆì–‘ì‹œë™ì•ˆêµ¬", "ê²½ê¸°ë„ ë¶€ì²œì‹œì›ë¯¸êµ¬", "ê²½ê¸°ë„ ë¶€ì²œì‹œì†Œì‚¬êµ¬",
        "ê²½ê¸°ë„ ë¶€ì²œì‹œì˜¤ì •êµ¬", "ê²½ê¸°ë„ ê´‘ëª…ì‹œ", "ê²½ê¸°ë„ í‰íƒì‹œ", "ê²½ê¸°ë„ ë™ë‘ì²œì‹œ",
        "ê²½ê¸°ë„ ì•ˆì‚°ì‹œìƒë¡êµ¬", "ê²½ê¸°ë„ ì•ˆì‚°ì‹œë‹¨ì›êµ¬", "ê²½ê¸°ë„ ê³ ì–‘ì‹œë•ì–‘êµ¬", "ê²½ê¸°ë„ ê³ ì–‘ì‹œì¼ì‚°ë™êµ¬",
        "ê²½ê¸°ë„ ê³ ì–‘ì‹œì¼ì‚°ì„œêµ¬", "ê²½ê¸°ë„ ê³¼ì²œì‹œ", "ê²½ê¸°ë„ êµ¬ë¦¬ì‹œ", "ê²½ê¸°ë„ ë‚¨ì–‘ì£¼ì‹œ",
        "ê²½ê¸°ë„ ì˜¤ì‚°ì‹œ", "ê²½ê¸°ë„ ì‹œí¥ì‹œ", "ê²½ê¸°ë„ êµ°í¬ì‹œ", "ê²½ê¸°ë„ ì˜ì™•ì‹œ", "ê²½ê¸°ë„ í•˜ë‚¨ì‹œ",
        "ê²½ê¸°ë„ ìš©ì¸ì‹œì²˜ì¸êµ¬", "ê²½ê¸°ë„ ìš©ì¸ì‹œê¸°í¥êµ¬", "ê²½ê¸°ë„ ìš©ì¸ì‹œìˆ˜ì§€êµ¬", "ê²½ê¸°ë„ íŒŒì£¼ì‹œ",
        "ê²½ê¸°ë„ ì´ì²œì‹œ", "ê²½ê¸°ë„ ì•ˆì„±ì‹œ", "ê²½ê¸°ë„ ê¹€í¬ì‹œ", "ê²½ê¸°ë„ í™”ì„±ì‹œ", "ê²½ê¸°ë„ ê´‘ì£¼ì‹œ",
        "ê²½ê¸°ë„ ì–‘ì£¼ì‹œ", "ê²½ê¸°ë„ í¬ì²œì‹œ", "ê²½ê¸°ë„ ì—¬ì£¼ì‹œ", "ê²½ê¸°ë„ ì—°ì²œêµ°", "ê²½ê¸°ë„ ê°€í‰êµ°",
        "ê²½ê¸°ë„ ì–‘í‰êµ°"
    ],
    "ì¸ì²œ": [
        "ì¸ì²œê´‘ì—­ì‹œ ì¤‘êµ¬", "ì¸ì²œê´‘ì—­ì‹œ ë™êµ¬", "ì¸ì²œê´‘ì—­ì‹œ ë¯¸ì¶”í™€êµ¬", "ì¸ì²œê´‘ì—­ì‹œ ì—°ìˆ˜êµ¬",
        "ì¸ì²œê´‘ì—­ì‹œ ë‚¨ë™êµ¬", "ì¸ì²œê´‘ì—­ì‹œ ë¶€í‰êµ¬", "ì¸ì²œê´‘ì—­ì‹œ ê³„ì–‘êµ¬", "ì¸ì²œê´‘ì—­ì‹œ ì„œêµ¬",
        "ì¸ì²œê´‘ì—­ì‹œ ê°•í™”êµ°", "ì¸ì²œê´‘ì—­ì‹œ ì˜¹ì§„êµ°"
    ],
    "ë¶€ì‚°": [
        "ë¶€ì‚°ê´‘ì—­ì‹œ ì¤‘êµ¬", "ë¶€ì‚°ê´‘ì—­ì‹œ ì„œêµ¬", "ë¶€ì‚°ê´‘ì—­ì‹œ ë™êµ¬", "ë¶€ì‚°ê´‘ì—­ì‹œ ì˜ë„êµ¬",
        "ë¶€ì‚°ê´‘ì—­ì‹œ ë¶€ì‚°ì§„êµ¬", "ë¶€ì‚°ê´‘ì—­ì‹œ ë™ë˜êµ¬", "ë¶€ì‚°ê´‘ì—­ì‹œ ë‚¨êµ¬", "ë¶€ì‚°ê´‘ì—­ì‹œ ë¶êµ¬",
        "ë¶€ì‚°ê´‘ì—­ì‹œ í•´ìš´ëŒ€êµ¬", "ë¶€ì‚°ê´‘ì—­ì‹œ ì‚¬í•˜êµ¬", "ë¶€ì‚°ê´‘ì—­ì‹œ ê¸ˆì •êµ¬", "ë¶€ì‚°ê´‘ì—­ì‹œ ê°•ì„œêµ¬",
        "ë¶€ì‚°ê´‘ì—­ì‹œ ì—°ì œêµ¬", "ë¶€ì‚°ê´‘ì—­ì‹œ ìˆ˜ì˜êµ¬", "ë¶€ì‚°ê´‘ì—­ì‹œ ì‚¬ìƒêµ¬", "ë¶€ì‚°ê´‘ì—­ì‹œ ê¸°ì¥êµ°"
    ],
    "ëŒ€êµ¬": [
        "ëŒ€êµ¬ê´‘ì—­ì‹œ ì¤‘êµ¬", "ëŒ€êµ¬ê´‘ì—­ì‹œ ë™êµ¬", "ëŒ€êµ¬ê´‘ì—­ì‹œ ì„œêµ¬", "ëŒ€êµ¬ê´‘ì—­ì‹œ ë‚¨êµ¬",
        "ëŒ€êµ¬ê´‘ì—­ì‹œ ë¶êµ¬", "ëŒ€êµ¬ê´‘ì—­ì‹œ ìˆ˜ì„±êµ¬", "ëŒ€êµ¬ê´‘ì—­ì‹œ ë‹¬ì„œêµ¬", "ëŒ€êµ¬ê´‘ì—­ì‹œ ë‹¬ì„±êµ°",
        "ëŒ€êµ¬ê´‘ì—­ì‹œ êµ°ìœ„êµ°"
    ],
    "ê´‘ì£¼": [
        "ê´‘ì£¼ê´‘ì—­ì‹œ ë™êµ¬", "ê´‘ì£¼ê´‘ì—­ì‹œ ì„œêµ¬", "ê´‘ì£¼ê´‘ì—­ì‹œ ë‚¨êµ¬", "ê´‘ì£¼ê´‘ì—­ì‹œ ë¶êµ¬", "ê´‘ì£¼ê´‘ì—­ì‹œ ê´‘ì‚°êµ¬"
    ],
    "ëŒ€ì „": [
        "ëŒ€ì „ê´‘ì—­ì‹œ ë™êµ¬", "ëŒ€ì „ê´‘ì—­ì‹œ ì¤‘êµ¬", "ëŒ€ì „ê´‘ì—­ì‹œ ì„œêµ¬", "ëŒ€ì „ê´‘ì—­ì‹œ ìœ ì„±êµ¬", "ëŒ€ì „ê´‘ì—­ì‹œ ëŒ€ë•êµ¬"
    ],
    "ìš¸ì‚°": [
        "ìš¸ì‚°ê´‘ì—­ì‹œ ì¤‘êµ¬", "ìš¸ì‚°ê´‘ì—­ì‹œ ë‚¨êµ¬", "ìš¸ì‚°ê´‘ì—­ì‹œ ë™êµ¬", "ìš¸ì‚°ê´‘ì—­ì‹œ ë¶êµ¬", "ìš¸ì‚°ê´‘ì—­ì‹œ ìš¸ì£¼êµ°"
    ],
    "ì„¸ì¢…": [
        "ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ ì„¸ì¢…ì‹œ"
    ],
    "ê°•ì›": [
        "ê°•ì›íŠ¹ë³„ìì¹˜ë„ ì¶˜ì²œì‹œ", "ê°•ì›íŠ¹ë³„ìì¹˜ë„ ì›ì£¼ì‹œ", "ê°•ì›íŠ¹ë³„ìì¹˜ë„ ê°•ë¦‰ì‹œ", "ê°•ì›íŠ¹ë³„ìì¹˜ë„ ë™í•´ì‹œ",
        "ê°•ì›íŠ¹ë³„ìì¹˜ë„ íƒœë°±ì‹œ", "ê°•ì›íŠ¹ë³„ìì¹˜ë„ ì†ì´ˆì‹œ", "ê°•ì›íŠ¹ë³„ìì¹˜ë„ ì‚¼ì²™ì‹œ", "ê°•ì›íŠ¹ë³„ìì¹˜ë„ í™ì²œêµ°",
        "ê°•ì›íŠ¹ë³„ìì¹˜ë„ íš¡ì„±êµ°", "ê°•ì›íŠ¹ë³„ìì¹˜ë„ ì˜ì›”êµ°", "ê°•ì›íŠ¹ë³„ìì¹˜ë„ í‰ì°½êµ°", "ê°•ì›íŠ¹ë³„ìì¹˜ë„ ì •ì„ êµ°",
        "ê°•ì›íŠ¹ë³„ìì¹˜ë„ ì² ì›êµ°", "ê°•ì›íŠ¹ë³„ìì¹˜ë„ í™”ì²œêµ°", "ê°•ì›íŠ¹ë³„ìì¹˜ë„ ì–‘êµ¬êµ°", "ê°•ì›íŠ¹ë³„ìì¹˜ë„ ì¸ì œêµ°",
        "ê°•ì›íŠ¹ë³„ìì¹˜ë„ ê³ ì„±êµ°", "ê°•ì›íŠ¹ë³„ìì¹˜ë„ ì–‘ì–‘êµ°"
    ],
    "ì¶©ë¶": [
        "ì¶©ì²­ë¶ë„ ì²­ì£¼ì‹œìƒë‹¹êµ¬", "ì¶©ì²­ë¶ë„ ì²­ì£¼ì‹œì„œì›êµ¬", "ì¶©ì²­ë¶ë„ ì²­ì£¼ì‹œí¥ë•êµ¬", "ì¶©ì²­ë¶ë„ ì²­ì£¼ì‹œì²­ì›êµ¬",
        "ì¶©ì²­ë¶ë„ ì¶©ì£¼ì‹œ", "ì¶©ì²­ë¶ë„ ì œì²œì‹œ", "ì¶©ì²­ë¶ë„ ë³´ì€êµ°", "ì¶©ì²­ë¶ë„ ì˜¥ì²œêµ°", "ì¶©ì²­ë¶ë„ ì˜ë™êµ°",
        "ì¶©ì²­ë¶ë„ ì¦í‰êµ°", "ì¶©ì²­ë¶ë„ ì§„ì²œêµ°", "ì¶©ì²­ë¶ë„ ê´´ì‚°êµ°", "ì¶©ì²­ë¶ë„ ìŒì„±êµ°", "ì¶©ì²­ë¶ë„ ë‹¨ì–‘êµ°"
    ],
    "ì¶©ë‚¨": [
        "ì¶©ì²­ë‚¨ë„ ì²œì•ˆì‹œë™ë‚¨êµ¬", "ì¶©ì²­ë‚¨ë„ ì²œì•ˆì‹œì„œë¶êµ¬", "ì¶©ì²­ë‚¨ë„ ê³µì£¼ì‹œ", "ì¶©ì²­ë‚¨ë„ ë³´ë ¹ì‹œ", "ì¶©ì²­ë‚¨ë„ ì•„ì‚°ì‹œ",
        "ì¶©ì²­ë‚¨ë„ ì„œì‚°ì‹œ", "ì¶©ì²­ë‚¨ë„ ë…¼ì‚°ì‹œ", "ì¶©ì²­ë‚¨ë„ ê³„ë£¡ì‹œ", "ì¶©ì²­ë‚¨ë„ ë‹¹ì§„ì‹œ", "ì¶©ì²­ë‚¨ë„ ê¸ˆì‚°êµ°",
        "ì¶©ì²­ë‚¨ë„ ë¶€ì—¬êµ°", "ì¶©ì²­ë‚¨ë„ ì„œì²œêµ°", "ì¶©ì²­ë‚¨ë„ ì²­ì–‘êµ°", "ì¶©ì²­ë‚¨ë„ í™ì„±êµ°", "ì¶©ì²­ë‚¨ë„ ì˜ˆì‚°êµ°",
        "ì¶©ì²­ë‚¨ë„ íƒœì•ˆêµ°"
    ],
    "ì „ë¶": [
        "ì „ë¶íŠ¹ë³„ìì¹˜ë„ ì „ì£¼ì‹œì™„ì‚°êµ¬", "ì „ë¶íŠ¹ë³„ìì¹˜ë„ ì „ì£¼ì‹œë•ì§„êµ¬", "ì „ë¶íŠ¹ë³„ìì¹˜ë„ êµ°ì‚°ì‹œ", "ì „ë¶íŠ¹ë³„ìì¹˜ë„ ìµì‚°ì‹œ",
        "ì „ë¶íŠ¹ë³„ìì¹˜ë„ ì •ìì‹œ", "ì „ë¶íŠ¹ë³„ìì¹˜ë„ ë‚¨ì›ì‹œ", "ì „ë¶íŠ¹ë³„ìì¹˜ë„ ê¹€ì œì‹œ", "ì „ë¶íŠ¹ë³„ìì¹˜ë„ ì™„ì£¼êµ°",
        "ì „ë¶íŠ¹ë³„ìì¹˜ë„ ì§„ì•ˆêµ°", "ì „ë¶íŠ¹ë³„ìì¹˜ë„ ë¬´ì£¼êµ°", "ì „ë¶íŠ¹ë³„ìì¹˜ë„ ì¥ìˆ˜êµ°", "ì „ë¶íŠ¹ë³„ìì¹˜ë„ ì„ì‹¤êµ°",
        "ì „ë¶íŠ¹ë³„ìì¹˜ë„ ìˆœì°½êµ°", "ì „ë¶íŠ¹ë³„ìì¹˜ë„ ê³ ì°½êµ°", "ì „ë¶íŠ¹ë³„ìì¹˜ë„ ë¶€ì•ˆêµ°"
    ],
    "ì „ë‚¨": [
        "ì „ë¼ë‚¨ë„ ëª©í¬ì‹œ", "ì „ë¼ë‚¨ë„ ì—¬ìˆ˜ì‹œ", "ì „ë¼ë‚¨ë„ ìˆœì²œì‹œ", "ì „ë¼ë‚¨ë„ ë‚˜ì£¼ì‹œ", "ì „ë¼ë‚¨ë„ ê´‘ì–‘ì‹œ",
        "ì „ë¼ë‚¨ë„ ë‹´ì–‘êµ°", "ì „ë¼ë‚¨ë„ ê³¡ì„±êµ°", "ì „ë¼ë‚¨ë„ êµ¬ë¡€êµ°", "ì „ë¼ë‚¨ë„ ê³ í¥êµ°", "ì „ë¼ë‚¨ë„ ë³´ì„±êµ°",
        "ì „ë¼ë‚¨ë„ í™”ìˆœêµ°", "ì „ë¼ë‚¨ë„ ì¥í¥êµ°", "ì „ë¼ë‚¨ë„ ê°•ì§„êµ°", "ì „ë¼ë‚¨ë„ í•´ë‚¨êµ°", "ì „ë¼ë‚¨ë„ ì˜ì•”êµ°",
        "ì „ë¼ë‚¨ë„ ë¬´ì•ˆêµ°", "ì „ë¼ë‚¨ë„ í•¨í‰êµ°", "ì „ë¼ë‚¨ë„ ì˜ê´‘êµ°", "ì „ë¼ë‚¨ë„ ì¥ì„±êµ°", "ì „ë¼ë‚¨ë„ ì™„ë„êµ°",
        "ì „ë¼ë‚¨ë„ ì§„ë„êµ°", "ì „ë¼ë‚¨ë„ ì‹ ì•ˆêµ°"
    ],
    "ê²½ë¶": [
        "ê²½ìƒë¶ë„ í¬í•­ì‹œë‚¨êµ¬", "ê²½ìƒë¶ë„ í¬í•­ì‹œë¶êµ¬", "ê²½ìƒë¶ë„ ê²½ì£¼ì‹œ", "ê²½ìƒë¶ë„ ê¹€ì²œì‹œ", "ê²½ìƒë¶ë„ ì•ˆë™ì‹œ",
        "ê²½ìƒë¶ë„ êµ¬ë¯¸ì‹œ", "ê²½ìƒë¶ë„ ì˜ì£¼ì‹œ", "ê²½ìƒë¶ë„ ì˜ì²œì‹œ", "ê²½ìƒë¶ë„ ìƒì£¼ì‹œ", "ê²½ìƒë¶ë„ ë¬¸ê²½ì‹œ",
        "ê²½ìƒë¶ë„ ê²½ì‚°ì‹œ", "ê²½ìƒë¶ë„ ì˜ì„±êµ°", "ê²½ìƒë¶ë„ ì²­ì†¡êµ°", "ê²½ìƒë¶ë„ ì˜ì–‘êµ°", "ê²½ìƒë¶ë„ ì˜ë•êµ°",
        "ê²½ìƒë¶ë„ ì²­ë„êµ°", "ê²½ìƒë¶ë„ ê³ ë ¹êµ°", "ê²½ìƒë¶ë„ ì„±ì£¼êµ°", "ê²½ìƒë¶ë„ ì¹ ê³¡êµ°", "ê²½ìƒë¶ë„ ì˜ˆì²œêµ°",
        "ê²½ìƒë¶ë„ ë´‰í™”êµ°", "ê²½ìƒë¶ë„ ìš¸ì§„êµ°", "ê²½ìƒë¶ë„ ìš¸ë¦‰êµ°"
    ],
    "ê²½ë‚¨": [
        "ê²½ìƒë‚¨ë„ ì°½ì›ì‹œì˜ì°½êµ¬", "ê²½ìƒë‚¨ë„ ì°½ì›ì‹œì„±ì‚°êµ¬", "ê²½ìƒë‚¨ë„ ì°½ì›ì‹œë§ˆì‚°í•©í¬êµ¬", "ê²½ìƒë‚¨ë„ ì°½ì›ì‹œë§ˆì‚°íšŒì›êµ¬",
        "ê²½ìƒë‚¨ë„ ì°½ì›ì‹œì§„í•´êµ¬", "ê²½ìƒë‚¨ë„ ì§„ì£¼ì‹œ", "ê²½ìƒë‚¨ë„ í†µì˜ì‹œ", "ê²½ìƒë‚¨ë„ ì‚¬ì²œì‹œ", "ê²½ìƒë‚¨ë„ ê¹€í•´ì‹œ",
        "ê²½ìƒë‚¨ë„ ë°€ì–‘ì‹œ", "ê²½ìƒë‚¨ë„ ê±°ì œì‹œ", "ê²½ìƒë‚¨ë„ ì–‘ì‚°ì‹œ", "ê²½ìƒë‚¨ë„ ì˜ë ¹êµ°", "ê²½ìƒë‚¨ë„ í•¨ì•ˆêµ°",
        "ê²½ìƒë‚¨ë„ ì°½ë…•êµ°", "ê²½ìƒë‚¨ë„ ê³ ì„±êµ°", "ê²½ìƒë‚¨ë„ ë‚¨í•´êµ°", "ê²½ìƒë‚¨ë„ í•˜ë™êµ°", "ê²½ìƒë‚¨ë„ ì‚°ì²­êµ°",
        "ê²½ìƒë‚¨ë„ í•¨ì–‘êµ°", "ê²½ìƒë‚¨ë„ ê±°ì°½êµ°", "ê²½ìƒë‚¨ë„ í•©ì²œêµ°"
    ],
    "ì œì£¼": [
        "ì œì£¼íŠ¹ë³„ìì¹˜ë„ ì œì£¼ì‹œ", "ì œì£¼íŠ¹ë³„ìì¹˜ë„ ì„œê·€í¬ì‹œ", "ì œì£¼ë„",
        "ì œì£¼",
        "ì œì£¼ë„",
        "ì œì£¼íŠ¹ë³„ìì¹˜ë„"
    ]
}

# ---- ë‹¨ì¼ í‚¤ì›Œë“œ ë° 'OOì‹œ' ë³€í˜• ìë™ ì¶”ê°€ ----
# ê° í‘œì¤€ ì§€ì—­ëª… ìì²´(ì˜ˆ: 'ì„œìš¸', 'ê²½ê¸°')ì™€ í”íˆ ì“°ëŠ” 'â—‹â—‹ì‹œ' ë³€í˜•ì„ REGION_MAPPING ë¦¬ìŠ¤íŠ¸ì— ìë™ í¬í•¨ì‹œì¼œ
# ë‹¨ì¼ í‚¤ì›Œë“œ ì…ë ¥ë„ ì¸ì‹í•˜ë„ë¡ í™•ì¥í•©ë‹ˆë‹¤.
for std_region, names in REGION_MAPPING.items():
    # 1) ë‹¨ì¼ í‘œì¤€ ì§€ì—­ëª… ì¶”ê°€
    if std_region not in names:
        names.append(std_region)
    # 2) 'â—‹â—‹ì‹œ' ë³€í˜• ì¶”ê°€ (ë„ ë‹¨ìœ„ëŠ” ì œì™¸)
    if not std_region.endswith("ë„") and not std_region.endswith("ì‹œ"):
        si_variant = f"{std_region}ì‹œ"
        if si_variant not in names:
            names.append(si_variant)

# ì§€ì—­ ì´ë¦„ ì—­ë§¤í•‘ (ì˜ˆ: 'ì—¬ì£¼ì‹œ' â†’ 'ê²½ê¸°')
REVERSE_REGION_LOOKUP = {}
for std_region, full_names in REGION_MAPPING.items():
    for name in full_names:
        tokens = re.findall(r"[ê°€-í£]{2,}", name)
        for token in tokens:
            if token not in REVERSE_REGION_LOOKUP:
                REVERSE_REGION_LOOKUP[token] = std_region

            # â€˜ì—¬ì£¼ì‹œâ€™ â†’ â€˜ì—¬ì£¼â€™ì²˜ëŸ¼ ì ‘ë¯¸ì‚¬(ì‹œÂ·êµ°Â·êµ¬) ì œê±° ë²„ì „ë„ ë§¤í•‘
            core_tok = re.sub(r"(ì‹œ|êµ°|êµ¬)$", "", token)
            if core_tok and core_tok not in REVERSE_REGION_LOOKUP:
                REVERSE_REGION_LOOKUP[core_tok] = std_region

        # ì „ì²´ ëª…ì¹­ë„ ì§ì ‘ ë§¤í•‘
        if name not in REVERSE_REGION_LOOKUP:
            REVERSE_REGION_LOOKUP[name] = std_region

# ì¶”ê°€: ë‹¨ì¼ ì§€ëª… í† í° ë§¤í•‘
REVERSE_REGION_LOOKUP.setdefault("ì œì£¼", "ì œì£¼")
REVERSE_REGION_LOOKUP.setdefault("ì œì£¼ë„", "ì œì£¼")
REVERSE_REGION_LOOKUP.setdefault("ì„œìš¸", "ì„œìš¸")
REVERSE_REGION_LOOKUP.setdefault("ì„œìš¸ì‹œ", "ì„œìš¸")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
# 2. ì •ì±… í‚¤ì›Œë“œ Â· ì¹´í…Œê³ ë¦¬
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
KEYWORDS = [
    "ë°”ìš°ì²˜", "í•´ì™¸ì§„ì¶œ", "ì¥ê¸°ë¯¸ì·¨ì—…ì²­ë…„", "ë§ì¶¤í˜•ìƒë‹´ì„œë¹„ìŠ¤", "êµìœ¡ì§€ì›",
    "ì¶œì‚°", "ë³´ì¡°ê¸ˆ", "ì¤‘ì†Œê¸°ì—…", "ë²¤ì²˜", "ëŒ€ì¶œ", "ê¸ˆë¦¬í˜œíƒ",
    "ì¸í„´", "ê³µê³µì„ëŒ€ì£¼íƒ", "ìœ¡ì•„", "ì²­ë…„ê°€ì¥", "ì‹ ìš©íšŒë³µ"
]

CATEGORIES = ["ì¼ìë¦¬", "ë³µì§€ë¬¸í™”", "ì°¸ì—¬ê¶Œë¦¬", "êµìœ¡", "ì£¼ê±°"]

INTEREST_EXPANSION = {
    "ìš´ë™": ["ìƒí™œì²´ìœ¡", "ìš´ë™ì²˜ë°©", "ìš´ë™ìš©í’ˆ ëŒ€ì—¬", "ê±´ê°•", "ì²´ë ¥", "í—¬ìŠ¤"],
    "ì°½ì—…": ["ì°½ì—…ì§€ì›", "ì°½ì—…êµìœ¡", "ì‚¬ì—…ìë“±ë¡", "ìŠ¤íƒ€íŠ¸ì—…"],
    "ì·¨ì—…": ["ì¼ìë¦¬", "ì§ë¬´êµìœ¡", "ì¸í„´ì‹­", "ì¼ê²½í—˜", "ì²­ë…„ê³ ìš©"],
    "ì£¼ê±°": ["ì„ëŒ€", "ì²­ë…„ì£¼íƒ", "ë³´ì¦ê¸ˆì§€ì›", "ì „ì„¸", "ì›”ì„¸"],
    "ë³µì§€": ["ì‹¬ë¦¬ìƒë‹´", "ì •ì‹ ê±´ê°•", "ê±´ê°•ê²€ì§„", "ìƒí™œë¹„ì§€ì›"]
}

def extract_keywords(text: str) -> List[str]:
    """ì‚¬ì „ í‚¤ì›Œë“œ + í•œê¸€ í˜•íƒœì†Œ ê¸°ë°˜ ê°„ì´ ì¶”ì¶œ"""
    hits = [kw for kw in KEYWORDS if kw in text]
    # ë³´ê°•: 2ê¸€ì ì´ìƒ ëª…ì‚¬ ë¹ˆë„ìˆ˜ ìƒìœ„ 5ê°œ ìë™ ì¶”ì¶œ(ê°„ë‹¨ regex)
    tokens = re.findall(r"[ê°€-í£]{2,}", text)
    freq = {}
    for t in tokens:
        freq[t] = freq.get(t, 0) + 1
    sorted_extra = sorted((w for w in freq if w not in hits),
                          key=lambda w: freq[w],
                          reverse=True)[:5]
    return hits + sorted_extra

def extract_categories(cat_field: str) -> List[str]:
    """
    ì •ì±… JSONì˜ category í•„ë“œ(ì‰¼í‘œ êµ¬ë¶„ í…ìŠ¤íŠ¸)ë¥¼ ê·¸ëŒ€ë¡œ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜.
    ì‚¬ì „ì— ì •ì˜ëœ CATEGORIESì— ì—†ë”ë¼ë„ ì €ì¥í•´ ë‘ê³ , í•„í„° ë‹¨ê³„ì—ì„œ ë§¤ì¹­í•©ë‹ˆë‹¤.
    """
    if not cat_field:
        return []
    return [c.strip() for c in cat_field.split(",") if c.strip()]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
# 3. ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ/ìƒì„± (ê°•í™” ë²„ì „)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
def load_or_build_vectorstore(json_path: str,
                              persist_dir: str,
                              api_key: str) -> Chroma:
    os.environ["OPENAI_API_KEY"] = api_key
    embedding = OpenAIEmbeddings()

    if os.path.exists(persist_dir) and os.listdir(persist_dir):
        return Chroma(persist_directory=persist_dir, embedding_function=embedding)

    with open(json_path, encoding="utf-8") as f:
        policies = json.load(f)

    def safe_int(val, default=0):
        try:
            return int(val)
        except (ValueError, TypeError):
            return default

    splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)
    vectordb = Chroma(persist_directory=persist_dir, embedding_function=embedding)

    for p in tqdm(policies, desc="Vectorizing policies"):
        text = (
            f"ì •ì±…ëª…: {p['title']}\n"
            f"ì •ì±…ID: {p.get('policy_id')}\n"
            f"ì§€ì›ëŒ€ìƒ: {safe_int(p.get('min_age'))}ì„¸~{safe_int(p.get('max_age'), 99)}ì„¸ / "
            f"ì§€ì—­ {', '.join(p.get('region_name', []))}\n"
            f"ì†Œë“ ë¶„ìœ„: {p.get('income_condition', 'ì œí•œ ì—†ìŒ')}\n"
            f"í˜œíƒ: {p.get('support_content', '')}\n"
            f"ì‹ ì²­ë°©ë²•: {p.get('apply_method', '')}\n"
            f"ì„¤ëª…: {p.get('description', '')}\n"
            f"ë§í¬: {p.get('apply_url', '')}"
        )
    
        existing_keywords = p.get("keywords", "")
        if isinstance(existing_keywords, str):
            existing_keywords = [kw.strip() for kw in existing_keywords.split(",") if kw.strip()]
        merged_keywords = list(set(existing_keywords + extract_keywords(text)))
        metadata = {
            "policy_id":        p.get("policy_id"),
            "title":            p["title"],
            "region":           ", ".join(p.get("region_name", [])),
            "categories":       ", ".join(extract_categories(p.get('category', ''))),
            "keywords":         ", ".join(merged_keywords),
            "min_age":          safe_int(p.get("min_age")),
            "max_age":          safe_int(p.get("max_age"), 99),
            "income_condition": p.get("income_condition", "ì œí•œ ì—†ìŒ"),
            "summary": (p.get("support_content") or p.get("description", ""))[:200],
            "apply_period":     p.get("apply_period", ""),
            "apply_url":        p.get("apply_url", ""),
        }

        # Ensure metadata values are primitive types for Chroma
        for mk, mv in metadata.items():
            if isinstance(mv, (list, set)):
                metadata[mk] = ", ".join(map(str, mv))

        chunks = splitter.split_text(text)
        documents = [Document(page_content=chunk, metadata=metadata) for chunk in chunks]
        vectordb.add_documents(documents)

    vectordb.persist()
    return vectordb

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
# 4. ì‚¬ìš©ì ì…ë ¥ íŒŒì‹±
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
from typing import Tuple, Optional, List

# ì‚¬ìš©ì ì…ë ¥ í´ë¦°ì—… í•¨ìˆ˜ ì¶”ê°€
import re
def clean_user_input(text: str) -> str:
    # Remove common conversational endings and particles that interfere with matching
    return re.sub(r"(ì—\s*ì‚¬ëŠ”?|ì•¼|ì¸ë°|ì´ì•¼|ì„|ì…ë‹ˆë‹¤|ê±°ë“ |ì„ë‹¤|ë¼êµ¬|ë¼ê³ )", "", text)

# ì¡°ì‚¬ ë“±ì„ ì œê±°í•˜ê³  í•µì‹¬ ë‹¨ì–´(ì˜ˆ: 'ì—¬ì£¼ì—' â†’ 'ì—¬ì£¼') ì¶”ì¶œ
def normalize_korean_tokens(text: str) -> List[str]:
    """
    ì¡°ì‚¬Â·í–‰ì •êµ¬ì—­ ì ‘ë¯¸ì‚¬(ì‹œÂ·êµ°Â·êµ¬) ì œê±° í›„ í•µì‹¬ ë‹¨ì–´ ì¶”ì¶œ
    """
    tokens = re.findall(r"[ê°€-í£]{2,}", text)
    normalized = []
    for tok in tokens:
        # ì¡°ì‚¬ ì œê±°
        core = re.sub(r"(ì—|ì—ì„œ|ì—ê²Œ|ë¡œ|ìœ¼ë¡œ|ì˜|ë¥¼|ì„|ì´|ê°€|ì€|ëŠ”|ë„|ë§Œ|ì´ë‚˜|ê¹Œì§€|ë¶€í„°)$", "", tok)
        # í–‰ì •êµ¬ì—­ ì ‘ë¯¸ì‚¬ ì œê±°
        core = re.sub(r"(ì‹œ|êµ°|êµ¬)$", "", core)
        if core and core not in normalized:
            normalized.append(core)
    return normalized

# ì§€ì—­ ì¶”ì¶œ ë³´ì¡° í•¨ìˆ˜
def extract_region(user_input: str, REGION_MAPPING: dict) -> str:
    cleaned_input = clean_text_for_matching(user_input)
    for std_region, keywords in REGION_MAPPING.items():
        for keyword in keywords:
            if keyword in cleaned_input:
                return std_region
    return ""

def parse_user_input(text: str) -> Tuple[Optional[int], Optional[str], Optional[List[str]]]:
    # í…ìŠ¤íŠ¸ ì •ê·œí™”: ì•ë¶€ë¶„ì—ì„œ strip ë° íŠ¹ìˆ˜ë¬¸ì ì œê±°
    text = text.strip()
    text = re.sub(r'[^\w\s]', '', text)  # Remove punctuation
    # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬: ì¡°ì‚¬ ì œê±° ë° ê³µë°± ì •ë¦¬
    text = re.sub(r"[^\wê°€-í£]", " ", text)
    text = re.sub(r"\s+", " ", text).strip()

    age = None
    # â‘  '26ì‚´', '26 ì„¸' í˜•íƒœ
    if m := re.search(r"(?:ë§Œ\s*)?(\d{1,2})\s*(?:ì„¸|ì‚´)", text):
        age = int(m.group(1))
    # â‘¡ ë‹¨ì¼ ìˆ«ìë§Œ ìˆëŠ” ê²½ìš°ë„ ë‚˜ì´ë¡œ ê°„ì£¼ (15~39ì„¸ ë²”ìœ„)
    if age is None:
        m2 = re.search(r"\b(\d{1,2})\b", text)
        if m2:
            age_cand = int(m2.group(1))
            if 15 <= age_cand <= 39:
                age = age_cand

    # ì§€ì—­ ì¶”ì¶œ ë¶€ë¶„ êµì²´: REGION_MAPPINGì˜ ëª¨ë“  ì‹œ/êµ°/êµ¬ ì´ë¦„ì´ í¬í•¨ë˜ë„ë¡ í™•ì¥ë˜ì–´ ìˆì–´ì•¼ í•¨
    region = ""
    for std_region, keywords in REGION_MAPPING.items():
        if any(keyword in text for keyword in keywords):
            region = std_region
            break
    # Fallback: ë‹¨ì¼ í† í° ê¸°ë°˜ ì§€ì—­ ì¶”ì¶œ
    if not region:
        for token in normalize_korean_tokens(text):
            if token in REVERSE_REGION_LOOKUP:
                region = REVERSE_REGION_LOOKUP[token]
                break

    interests = None
    matches = [std_i for std_i, kws in INTEREST_MAPPING.items() if any(k in text for k in kws)]
    if matches:
        interests = matches

    return age, region, interests

# 5. ì •ë³´ ëˆ„ë½ í™•ì¸ í•¨ìˆ˜ ì¶”ê°€
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
def missing_info(age, region, interests) -> List[str]:
    needs = []
    if age is None:
        needs.append("ë‚˜ì´")
    if region is None:
        needs.append("ì§€ì—­")
    if not interests or len(interests) == 0:
        needs.append("ê´€ì‹¬ì‚¬")
    return needs

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
# ğŸ”§ ì¶”ì²œ ê°€ëŠ¥í•œ ê´€ì‹¬ì‚¬ ë¦¬ìŠ¤íŠ¸ í—¬í¼
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
def suggest_remaining_interests(current: List[str]) -> str:
    """
    í˜„ì¬ stored_interests ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì•„ì§ ì œì•ˆí•˜ì§€ ì•Šì€
    INTEREST_MAPPING ìƒìœ„ ì¹´í…Œê³ ë¦¬ë¥¼ ì½¤ë§ˆë¡œ ë‚˜ì—´í•´ ë°˜í™˜.
    5ê°œê¹Œì§€ë§Œ ë³´ì—¬ì£¼ê³  ë‚˜ë¨¸ì§€ëŠ” 'ë“±'ìœ¼ë¡œ í‘œê¸°.
    """
    remaining = [k for k in INTEREST_MAPPING.keys() if k not in current]
    shown = remaining[:5]
    suggestion = ", ".join(shown)
    if len(remaining) > 5:
        suggestion += " ë“±"
    return suggestion


def classify_user_type(text: str) -> str:
    known = ["ì²­ë…„ë‚´ì¼ì±„ì›€ê³µì œ", "ë„ì•½ê³„ì¢Œ", "êµ¬ì§í™œë™ì§€ì›ê¸ˆ", "êµ­ë¯¼ì·¨ì—…ì§€ì›ì œë„", "ì •ì±…ëª…"]
    return "policy_expert" if any(kw in text for kw in known) else "policy_novice"
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
# 5. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
SYSTEM = SystemMessagePromptTemplate.from_template("""
[ROLE]
ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ë§Œ 19~39ì„¸ ì²­ë…„ì„ ìœ„í•œ ì •ì±… ì•ˆë‚´ ì±—ë´‡ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì…ë ¥ê³¼ ì œê³µëœ context ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ, í•´ë‹¹ ì²­ë…„ì—ê²Œ ê°€ì¥ ì í•©í•œ ì •ì±…ì„ ì°¾ì•„ ì•ˆë‚´í•˜ëŠ” ì—­í• ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

[TASK - Chain of Thought ë°©ì‹]
ì‚¬ìš©ìì˜ ì¡°ê±´(ë‚˜ì´, ì§€ì—­, ê´€ì‹¬ì‚¬)ì„ ë°”íƒ•ìœ¼ë¡œ ì•„ë˜ ìˆœì„œëŒ€ë¡œ ì¶”ë¡ í•˜ë©° ì •ì±…ì„ ì¶”ì²œí•˜ì„¸ìš”:

1. ë¨¼ì € ì‚¬ìš©ìì˜ ë‚˜ì´ê°€ ê° ì •ì±…ì˜ ë‚˜ì´ ì¡°ê±´(min_age~max_age)ì— ë¶€í•©í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
2. ë‹¤ìŒìœ¼ë¡œ ì§€ì—­ ì¡°ê±´ì´ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤. ì •í™•í•œ ì§€ì—­ì´ ì—†ìœ¼ë©´ ì „êµ­ ê³µí†µ ì •ì±…ì„ í¬í•¨í•©ë‹ˆë‹¤.
3. ê´€ì‹¬ì‚¬ ë˜ëŠ” ì„¸ë¶€ ê´€ì‹¬ì‚¬ê°€ ì •ì±… í‚¤ì›Œë“œ ë˜ëŠ” ì„¤ëª…ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ íŒë‹¨í•©ë‹ˆë‹¤.
4. ìœ„ ì¡°ê±´ë“¤ì— ê¸°ë°˜í•´ ì í•©í•œ ì •ì±…ì„ ìš°ì„ ìˆœìœ„ë¡œ ì •ë ¬í•œ í›„, ìƒìœ„ 3ê±´ì„ ì¶”ì²œí•©ë‹ˆë‹¤.
5. ê° ì •ì±…ì€ ì¶”ì²œ ì´ìœ (ë‚˜ì´/ì§€ì—­/ê´€ì‹¬ì‚¬ ì¡°ê±´ì— ì–´ë–»ê²Œ ë¶€í•©í•˜ëŠ”ì§€)ë¥¼ í•œ ì¤„ë¡œ ì„¤ëª…í•´ ì£¼ì„¸ìš”.
6. ì¡°ê±´ì´ ëª…í™•í•˜ì§€ ì•Šìœ¼ë©´ ì¡°íšŒëŸ‰ì´ ë§ì€ ì „êµ­ ê³µí†µ ì •ì±… 3ê±´ì„ ëŒ€ì‹  ì¶”ì²œí•˜ì„¸ìš”.

[OUTPUT FORMAT - MARKDOWN]
- ì •ì±…ëª… (ì†Œë“: â—‹â—‹): ì§€ì›ë‚´ìš© ìš”ì•½ â€” ì¶”ì²œ ì´ìœ  (ë§í¬ : apply_url) (ì •ì²µID : policy_id)
- ì •ì±…ëª… (ì†Œë“: â—‹â—‹): ì§€ì›ë‚´ìš© ìš”ì•½ â€” ì¶”ì²œ ì´ìœ  (ë§í¬ : apply_url) (ì •ì²µID : policy_id)
- ì •ì±…ëª… (ì†Œë“: â—‹â—‹): ì§€ì›ë‚´ìš© ìš”ì•½ â€” ì¶”ì²œ ì´ìœ  (ë§í¬ : apply_url) (ì •ì²µID : policy_id)

[EXCEPTION]
- ì¡°ê±´ì— ë§ëŠ” ì •ì±…ì´ ì—†ì„ ê²½ìš°:
    ëŒ€ì‹  ì „êµ­ ê³µí†µ ì •ì±… 3ê±´ì„ ì¶œë ¥í•˜ì„¸ìš”.

[EXAMPLE - NORMAL]
- ì²­ë…„ë‚´ì¼ì±„ì›€ê³µì œ (ì†Œë“: ì œí•œ ì—†ìŒ): ì¤‘ì†Œê¸°ì—… ê·¼ë¬´ ì²­ë…„ì—ê²Œ ëª©ëˆ ë§ˆë ¨ ì§€ì› â€” ë‚˜ì´ì™€ ì†Œë“ ì¡°ê±´ ëª¨ë‘ ë¶€í•© (ì¶œì²˜: policy_123)
- êµ­ë¯¼ì·¨ì—…ì§€ì›ì œë„ (ì†Œë“: ê¸°ì¤€ì¤‘ìœ„ì†Œë“ 100% ì´í•˜): ì·¨ì—…ì¤€ë¹„ ì¤‘ ì²­ë…„ì—ê²Œ ë§ì¶¤í˜• ì·¨ì—…ì§€ì› â€” ê´€ì‹¬ì‚¬ 'ì·¨ì—…'ê³¼ ì¼ì¹˜ (ì¶œì²˜: policy_456)
- ì²­ë…„êµ¬ì§í™œë™ì§€ì›ê¸ˆ (ì†Œë“: ê¸°ì¤€ì¤‘ìœ„ì†Œë“ 120% ì´í•˜): êµ¬ì§í™œë™ë¹„ ì›” ìµœëŒ€ 50ë§Œì› ì§€ì› â€” ì§€ì—­, ê´€ì‹¬ì‚¬ ëª¨ë‘ ì¼ì¹˜ (ì¶œì²˜: policy_789)

[EXAMPLE - FALLBACK]
í•´ë‹¹ ì¡°ê±´ì— ë§ëŠ” ì •ì±…ì´ ì—†ìŠµë‹ˆë‹¤. ëŒ€ì‹  ì „êµ­ ê³µí†µ ì •ì±… 3ê±´ì„ ì¶”ì²œí•©ë‹ˆë‹¤.

[EXAMPLE - ASK INFO]
ë‚˜ì´ ë˜ëŠ” ì§€ì—­ ì •ë³´ë¥¼ ì•Œë ¤ì£¼ì‹œë©´ ë”ìš± ì •í™•í•œ ì¶”ì²œì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.
""")

combine_prompt = ChatPromptTemplate.from_messages([
    SYSTEM,
    HumanMessagePromptTemplate.from_template(
        "context:\n{context}\n\nì§ˆë¬¸: {question}\n\ní•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”."
    ),
])

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
# 6. RAG ì²´ì¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
def create_rag_chain(vectordb: Chroma, api_key: str) -> ConversationalRetrievalChain:
    llm = ChatOpenAI(model_name="gpt-4o", temperature=0)
    retriever = vectordb.as_retriever(search_kwargs={"k": 30})
    memory = ConversationBufferMemory(
        memory_key="chat_history",
        input_key="question",
        output_key="answer",  
        return_messages=True
    )
    chain =  ConversationalRetrievalChain.from_llm(
        llm, retriever, memory=memory,
        combine_docs_chain_kwargs={"prompt": combine_prompt, "document_variable_name": "context"},
        output_key="answer", return_source_documents=True
    )
    return chain, llm

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
# 7. ê°€ì¤‘ì¹˜ í•„í„° & í´ë°±
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
# ì„ í˜• ê°€ì¤‘í•© ëª¨ë¸ ê¸°ë°˜ í•„í„°ë§
# ê°€ì¤‘ì¹˜: ì§€ì—­ 0.6(ì „êµ­ í¬í•¨), ê´€ì‹¬ì‚¬ 0.35, í‚¤ì›Œë“œ 0.05
MIN_SCORE = 0.3  # ì´í•© 1.0 ì¤‘ 0.3 ì´ìƒì´ë©´ ì±„íƒ

W_REGION   = 0.6
W_INTEREST = 0.35
W_KEYWORD  = 0.05


def jaccard_similarity(a: set, b: set) -> float:
    """ë‘ ì§‘í•©ì˜ ìì¹´ë“œ ìœ ì‚¬ë„(0~1). ê³µì§‘í•©ì´ë©´ 0."""
    if not a or not b:
        return 0.0
    return len(a & b) / len(a | b)


def filter_docs(docs,user_age: int, user_text: str, region: str, interests: List[str]):
    """
    docs        : LangChain Document ë¦¬ìŠ¤íŠ¸
    user_age    : ë‚˜ì´ ì¡°ê±´
    user_text   : ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì›ë¬¸
    region      : íŒŒì‹±ëœ í‘œì¤€ ì§€ì—­(ì˜ˆ: 'ì„œìš¸')
    interests   : íŒŒì‹±ëœ ê´€ì‹¬ì‚¬ ë¦¬ìŠ¤íŠ¸(ì˜ˆ: ['ì°½ì—…', 'ì£¼ê±°'])
    """
    filtered = []
    kw_hits = extract_keywords(user_text)          # ì‚¬ìš©ì ë¬¸ì¥ì—ì„œ ì¶”ì¶œëœ í‚¤ì›Œë“œ ì§‘í•©
    interests_set = set(interests)

    for d in docs:
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
        # 0. ë‚˜ì´ í•„í„° : ë©”íƒ€ë°ì´í„°ê°€ ì—†ë‹¤ë©´ í†µê³¼
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
        min_age = d.metadata.get("min_age", 0)
        max_age = d.metadata.get("max_age",999)
        if user_age not in range(min_age, max_age + 1):
            continue

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
        # 1. ì§€ì—­ ì ìˆ˜ (R: 0 | 0.5 | 1)
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
        doc_region_str = d.metadata.get("region", "")
        is_nationwide = ("ì „êµ­" in doc_region_str) or (doc_region_str.strip() == "")
        if is_nationwide:
            region_score = 1.0  # ì „êµ­ ì •ì±…ì€ ë™ì¼ ê°€ì¤‘ì¹˜
        elif region and any(k in doc_region_str for k in REGION_MAPPING.get(region, [])):
            region_score = 1.0
        elif region and region in doc_region_str:          # ëŠìŠ¨í•œ í¬í•¨
            region_score = 0.5
        else:
            region_score = 0.0

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
        # 2. ê´€ì‹¬ì‚¬ ì ìˆ˜ (I: 0~1)
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
        # 'categories'ê°€ str(list) í˜•íƒœë¡œ ë“¤ì–´ì˜¬ ìˆ˜ë„ ìˆì–´ íŒŒì‹± ì§„í–‰
        cat_raw = d.metadata.get("categories", [])
        if isinstance(cat_raw, str):
            cat_tokens = [c.strip() for c in re.split(r"[,\[\]'\"\s]+", cat_raw) if c.strip()]
        else:
            cat_tokens = cat_raw
        policy_tags = set(cat_tokens)

        if policy_tags:
            interest_score = jaccard_similarity(interests_set, policy_tags)
        else:
            # ì¹´í…Œê³ ë¦¬ê°€ ë¹„ì–´ ìˆìœ¼ë©´ ë¬¸ì„œ ë³¸ë¬¸ì— ê´€ì‹¬ì‚¬ í‚¤ì›Œë“œê°€ ì§ì ‘ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ ê³„ì‚°
            hits = sum(1 for i in interests_set if i in d.page_content)
            interest_score = hits / len(interests_set) if interests_set else 0.0

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
        # 3. í‚¤ì›Œë“œ ì ìˆ˜ (K: 0~1)
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
        if kw_hits:
            key_raw = d.metadata.get("keywords", [])
            if isinstance(key_raw, str):
                key_tokens = [k.strip() for k in re.split(r"[,\[\]'\"\s]+", key_raw) if k.strip()]
            else:
                key_tokens = key_raw
            doc_keywords = set(key_tokens)
            keyword_score = len(doc_keywords & set(kw_hits)) / len(kw_hits)
        else:
            keyword_score = 0.0

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
        # 4. ìµœì¢… ì ìˆ˜ (ë™ì  ê°€ì¤‘ì¹˜)
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
        total_w = 0
        score_sum = 0
        if region:
            total_w += W_REGION
            score_sum += W_REGION * region_score
        if interests_set:
            total_w += W_INTEREST
            score_sum += W_INTEREST * interest_score
        if kw_hits:
            total_w += W_KEYWORD
            score_sum += W_KEYWORD * keyword_score
        # ëª¨ë“  í•­ëª©ì´ ë¹„ì–´ ìˆìœ¼ë©´ í‚¤ì›Œë“œë§Œì´ë¼ë„ ì‚¬ìš©
        if total_w == 0:
            total_w = W_KEYWORD
            score_sum = W_KEYWORD * keyword_score
        score = score_sum / total_w

        # ë””ë²„ê¹…ìš© ì ìˆ˜ ë©”íƒ€ë°ì´í„° ì €ì¥
        d.metadata["debug_region_score"]   = round(region_score,   3)
        d.metadata["debug_interest_score"] = round(interest_score, 3)
        d.metadata["debug_keyword_score"]  = round(keyword_score,  3)
        d.metadata["debug_total_score"]    = round(score,         3)

        if score >= MIN_SCORE:
            filtered.append((score, d))
        
    # ì ìˆ˜ ë†’ì€ ìˆœ ì •ë ¬ í›„ Document ë¦¬ìŠ¤íŠ¸ë§Œ ë°˜í™˜
    return [d for _, d in sorted(filtered, key=lambda x: x[0], reverse=True)]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
# 9. ê´€ì‹¬ì‚¬ ì„¸ë¶€ ë¶„ë¥˜ íë¦„ ìœ ë„ (LLM ê¸°ë°˜)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
SUB_INTEREST_MAPPING = {
    "ì·¨ì—…": {
        "ë©´ì ‘ì¤€ë¹„": ["ëª¨ì˜ë©´ì ‘", "ë©´ì ‘ë³µì¥", "ì´ë ¥ì„œ í´ë¦¬ë‹‰", "ì¦ëª…ì‚¬ì§„", "ì •ì¥ ëŒ€ì—¬"],
        "ì—­ëŸ‰ê°•í™”": ["ì§ì—…í›ˆë ¨", "ì§ë¬´êµìœ¡", "ì·¨ì—…ê¸°ìˆ  í–¥ìƒ", "ì¡ì¼€ì–´", "ìê²©ì¦"],
        "í˜„ì¥ê²½í—˜": ["ì¼ ê²½í—˜", "ì¸í„´ì‹­", "í˜„ì¥ì‹¤ìŠµ", "ê¸°ì—… ì—°ê³„ í”„ë¡œì íŠ¸"],
        "êµ¬ì§ì§€ì›ê¸ˆ": ["êµ¬ì§ì´‰ì§„ìˆ˜ë‹¹", "ì·¨ì—…ì„±ê³µìˆ˜ë‹¹", "ì·¨ì—…ì¥ë ¤ê¸ˆ", "í™œë™ë¹„ ì§€ì›"],
        "ê³ ìš©ì—°ê³„": ["ì±„ìš©ì—°ê³„", "ê³µê³µê¸°ê´€ ì±„ìš©", "ì²­ë…„ì±„ìš© ì—°ê³„ì‚¬ì—…"]
    },
    "ì°½ì—…": {
        "ë©˜í† ë§Â·ìƒë‹´": ["ì°½ì—…ìƒë‹´", "ì°½ì—…ì»¨ì„¤íŒ…", "BMëª¨ë¸", "ë²•ë¥ Â·íšŒê³„", "ì„¸ë¬´ì§€ì›"],
        "ì‚¬ì—…ê³„íšÂ·ê¸°íš": ["ì‚¬ì—…ê³„íšì„œ ì‘ì„±", "ì•„ì´ë””ì–´ ê³ ë„í™”", "ì°½ì—… R&D", "ì•„ì´í…œ ë°œêµ´"],
        "ìê¸ˆì§€ì›": ["ê¸ˆë¦¬ì§€ì›", "ë³´ì¦ê¸ˆ", "ìœµì", "ì°½ì—…ìê¸ˆ"],
        "ì°½ì—…êµìœ¡": ["ì°½ì—… êµìœ¡", "ì°½ì—…í¬ëŸ¼", "ì°½ì—… ì•„ì¹´ë°ë¯¸", "ë„¤íŠ¸ì›Œí‚¹"]
    },
    "ìš´ë™": {
        "ê±´ê°•ê´€ë¦¬": ["í—¬ìŠ¤ì¼€ì–´", "ê±´ê°•ê²€ì§„", "ê±´ê°•ì„œë¹„ìŠ¤", "ì˜ë£Œì„œë¹„ìŠ¤"],
        "ì²´ìœ¡í™œë™": ["í”¼íŠ¸ë‹ˆìŠ¤", "ìš”ê°€", "ìŠ¤í¬ì¸ ì„¼í„°", "ì²´ìœ¡ê´€"],
        "ì •ì‹ ê±´ê°•": ["ì‹¬ë¦¬ìƒë‹´", "ì •ì„œì§€ì›", "ìŠ¤íŠ¸ë ˆìŠ¤ ì™„í™”", "ìš°ìš¸ì¦ ì§€ì›"]
    },
    "ì£¼ê±°": {
        "ì„ëŒ€ë£Œì§€ì›": ["ì›”ì„¸ì§€ì›", "ì„ëŒ€ë£Œ ë³´ì¡°", "ê³µê³µì„ëŒ€ì£¼íƒ", "ì£¼ê±°ë°”ìš°ì²˜"],
        "ì£¼íƒêµ¬ì…Â·ëŒ€ì¶œ": ["ì£¼íƒ ëŒ€ì¶œ", "ì „ì„¸ ëŒ€ì¶œ", "ë³´ì¦ê¸ˆ ì§€ì›"],
        "ì£¼íƒê°œë³´ìˆ˜": ["ì£¼íƒì •ë¹„", "ë¦¬ëª¨ë¸ë§", "ë¹ˆì§‘ í™œìš©"]
    }
}

# ì„¸ë¶€ ê´€ì‹¬ì‚¬ ì§ˆë¬¸ ìœ ë„ í•¨ìˆ˜ (ëŒ€í™”í˜• ë°©ì‹, ì˜ˆì‹œ ë™ì  ë°˜ì˜)
def prompt_sub_interest(main_interest: str) -> Optional[str]:
    sub_map = SUB_INTEREST_MAPPING.get(main_interest)
    if not sub_map:
        return None

    print(f"\nBot:\n{main_interest}ê³¼ ê´€ë ¨í•´ ì•„ë˜ì™€ ê°™ì€ ì§€ì›ì´ ìˆì–´ìš”:")
    suggestions = list(sub_map.keys())
    for idx, key in enumerate(suggestions, 1):
        example_keywords = ", ".join(sub_map[key][:2])
        print(f"- {key}: {example_keywords} ê´€ë ¨ ì§€ì›")

    example_hint = ", ".join(suggestions[:2])
    print(f"\níŠ¹ë³„íˆ ê¶ê¸ˆí•œ ê²ƒì´ ìˆìœ¼ì‹ ê°€ìš”? (ì˜ˆ: {example_hint} ë“±)")
    sel = input("ê´€ì‹¬ ìˆëŠ” ë‚´ìš©ì„ ì ì–´ì£¼ì„¸ìš”: ").strip()
    for key in suggestions:
        if key in sel:
            return key
    print("ì…ë ¥ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ íŠ¹ì • í•­ëª©ì„ ì°¾ì„ ìˆ˜ ì—†ì—ˆì–´ìš”. ì¼ë°˜ ì¶”ì²œì„ ì§„í–‰í• ê²Œìš”.")
    return None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
# 8. ì½˜ì†” ì±„íŒ…
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
def console_chat(rag_chain, llm, keyword_vectordb=None, category_vectordb=None, policy_vectordb=None):
    print("\nì±—ë´‡ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. ì¢…ë£Œí•˜ë ¤ë©´ 'ì¢…ë£Œ'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.\n")

    stored_age = None
    stored_region = None
    stored_interests = []
    # ì´ë¯¸ ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì¤€ ì •ì±…ID ì§‘í•©
    recommended_ids = set()

    # Ensure vectordb refers to main policy vectorstore
    vectordb = policy_vectordb if policy_vectordb is not None else policy_vectordb

    # â± total_response_time = 0 # ì‘ë‹µì‹œê°„ ê³„ì‚° ë‚˜ì¤‘ì— ì œê±°í•˜ê¸°
    # â± response_count = 0 # ì‘ë‹µ íšŸìˆ˜ ë‚˜ì¤‘ì— ì œê±°
    def is_new_topic(predicted: list[str], stored: list[str]) -> bool:
        return not any(kw in stored for kw in predicted)

    while True:
        user_input = input("You: ")
        if user_input.strip().lower() in ["ì¢…ë£Œ", "exit", "quit"]:
            print("Bot: ì´ìš©í•´ ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤. ì•ˆë…•íˆ ê°€ì„¸ìš”!")
            break


        # ì‚¬ìš©ìê°€ 'ë‹¤ë¥¸ ì •ì±…' ë“± ì¶”ê°€ ì¶”ì²œë§Œ ìš”ì²­í–ˆëŠ”ì§€ í”Œë˜ê·¸
        force_more_request = is_generic_more_request(user_input)

        # ì‚¬ìš©ìê°€ 'ì–´ë–¤ ë¶„ì•¼'ë¥¼ ë¬¼ìœ¼ë©´ í˜„ì¬ ê°€ëŠ¥í•œ ì¹´í…Œê³ ë¦¬ ì œì•ˆ
        if re.search(r"ì–´ë–¤\s*ë¶„ì•¼.*(ìˆ|ì•¼|ê°€)", user_input):
            suggestion = suggest_remaining_interests(stored_interests)
            print(f"Bot:\ní˜„ì¬ ì„ íƒí•  ìˆ˜ ìˆëŠ” ë¶„ì•¼ë¡œëŠ” {suggestion}ì´ ìˆìŠµë‹ˆë‹¤.\n")
            continue

        if not force_more_request and not is_policy_related_question_llm(user_input):
            print("Bot:\nì €ëŠ” ëŒ€í•œë¯¼êµ­ ì²­ë…„ ì •ì±… ì•ˆë‚´ë¥¼ ë„ì™€ë“œë¦¬ëŠ” ì±—ë´‡ì´ì—ìš”! ì •ì±… ê´€ë ¨ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš” ğŸ˜Š\n")
            continue

        # ìœ íš¨ ì§ˆë¬¸ì¸ì§€ ê²€ì‚¬ (ë‹¨, 'ë‹¤ë¥¸ ì •ì±…' ì¶”ê°€ ìš”ì²­ì€ ê±´ë„ˆëœ€)
        age, region, interests = parse_user_input(user_input)
        if not force_more_request and not is_valid_query(user_input) and not any([age, region, interests]):
            print("Bot:\nì•ˆë…•í•˜ì„¸ìš”! ê¶ê¸ˆí•˜ì‹  ì •ì±…ì´ë‚˜ ì¡°ê±´ì„ ì…ë ¥í•´ ì£¼ì„¸ìš” ğŸ™‚\n")
            continue

        # ì‚¬ìš©ì ì…ë ¥ì—ì„œ ìë™ ì •ë³´ ì¶”ì¶œ ë° ì¶œë ¥
        user_info = extract_user_info(user_input)
        print(f"[ğŸ§  ìë™ ì¶”ì¶œ ì •ë³´] ë‚˜ì´: {user_info['age']}, ì§€ì—­: {user_info['region']}, ê´€ì‹¬ì‚¬: {user_info['interests']}, ìƒíƒœ: {user_info['status']}, ì†Œë“: {user_info['income']}")

        # extract_user_infoì˜ ì¶œë ¥ê°’ì„ ê·¸ëŒ€ë¡œ ë°˜ì˜
        if user_info['age']:
            stored_age = user_info['age']
        if user_info['region']:
            stored_region = user_info['region']
        if user_info['interests']:
            stored_interests = user_info['interests']

        # ğŸ’¡ ì •ë³´ê°€ ëª¨ë‘ ì—†ìœ¼ë©´ ë°”ë¡œ ì•ˆë‚´í•˜ê³  ë‹¤ìŒ ì…ë ¥ ëŒ€ê¸°
        if not any([stored_age, stored_region, stored_interests]):
            print("Bot:\në‚˜ì´, ì§€ì—­, ê´€ì‹¬ì‚¬ ì •ë³´ë¥¼ ì•Œë ¤ì£¼ì‹œë©´ ë§ì¶¤í˜• ì •ì±…ì„ ì•ˆë‚´í•´ë“œë¦´ê²Œìš” ğŸ˜Š\n")
            continue

        # â± start_time = time.time()  # ì‘ë‹µ ì‹œê°„ ì¸¡ì • ì‹œì‘ (ìœ„ì¹˜ ì´ë™) # ì¶”í›„ ì œê±°

        # ê´€ì‹¬ì‚¬ ì¶”ë¡ 
        predicted_keywords = None
        embedding = None
        if keyword_vectordb:
            embedding = OpenAIEmbeddings()
            query_vector = embedding.embed_query(user_input)
            docs = keyword_vectordb.similarity_search_by_vector(query_vector, k=3)
            if docs:
                predicted_keywords = [doc.page_content for doc in docs]

        if not predicted_keywords and category_vectordb:
            if embedding is None:
                embedding = OpenAIEmbeddings()
            query_vector = embedding.embed_query(user_input)
            docs = category_vectordb.similarity_search_by_vector(query_vector, k=2)
            if docs:
                predicted_keywords = [doc.page_content for doc in docs]

        if not predicted_keywords:
            from langchain.prompts import PromptTemplate
            prompt = PromptTemplate.from_template("""
            [ì‹œìŠ¤í…œ]
            ë‹¤ìŒ ë¬¸ì¥ì—ì„œ ê´€ë ¨ ìˆëŠ” ê´€ì‹¬ì‚¬ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.
            ì„ íƒ ê°€ëŠ¥í•œ í•­ëª©: ì°½ì—…, ì·¨ì—…, ê¸ˆìœµ, ë³µì§€, êµìœ¡, ê³µê°„, ë¬¸í™”ì˜ˆìˆ 

            ë¬¸ì¥:
            {input}

            ê²°ê³¼:
            """)
            response = llm.invoke(prompt.format(input=user_input).to_messages())
            predicted_keywords = [i.strip() for i in response.content.split(",") if i.strip()]

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
        # ğŸ”§ ì˜ˆì¸¡ í‚¤ì›Œë“œ â†’ í‘œì¤€ ê´€ì‹¬ì‚¬ ë§¤í•‘ ê°œì„ 
        # - ë²¡í„°DBì—ì„œ ê°€ì ¸ì˜¨ 'ì œì£¼ì‹œ', 'ê±°ì£¼ì' ê°™ì€ í† í° ì œê±°
        # - INTEREST_MAPPINGì— ì •ì˜ëœ í‚¤ì›Œë“œë§Œ í‘œì¤€í™”í•´ ì‚¬ìš©
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
        if predicted_keywords:
            std_interests = []
            for kw in predicted_keywords:
                # ë¨¼ì €, ì§€ì—­ í‚¤ì›Œë“œëŠ” ì œì™¸
                if is_region_keyword(kw):
                    continue
                # í‘œì¤€ ê´€ì‹¬ì‚¬ëª… ê·¸ëŒ€ë¡œ ë“¤ì–´ì˜¨ ê²½ìš°
                if kw in INTEREST_MAPPING:
                    if kw not in std_interests:
                        std_interests.append(kw)
                    continue
                # í‚¤ì›Œë“œê°€ INTEREST_MAPPING í•˜ìœ„ í‚¤ì›Œë“œì— í¬í•¨ë˜ëŠ”ì§€ í™•ì¸
                for std_i, kws in INTEREST_MAPPING.items():
                    if kw in kws and std_i not in std_interests:
                        std_interests.append(std_i)
                        break
            # ë§¤í•‘ ê²°ê³¼ê°€ ì—†ë‹¤ë©´ ì˜ˆì¸¡ í‚¤ì›Œë“œ ë¬´ì‹œ
            predicted_keywords = std_interests if std_interests else None

        # ê´€ì‹¬ì‚¬ ì´ˆê¸°í™” ì¡°ê±´ ì²´í¬ ë° ì €ì¥
        if predicted_keywords:
            if is_new_topic(predicted_keywords, stored_interests):
                print("ğŸ§¹ ê¸°ì¡´ ê´€ì‹¬ì‚¬ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.")
                new_interests = predicted_keywords
            else:
                new_interests = stored_interests[:]
                for kw in predicted_keywords:
                    if kw not in new_interests:
                        new_interests.append(kw)

            # ì§€ì—­ëª…ì„ ê´€ì‹¬ì‚¬ì—ì„œ ì œê±°
            filtered_interests = [kw for kw in new_interests if not is_region_keyword(kw)]
            if filtered_interests:
                stored_interests = filtered_interests
            # stored_interests = new_interests  # ê¸°ì¡´ ì§ì ‘ ëŒ€ì…ì€ ì œê±°/ì£¼ì„ ì²˜ë¦¬

        print(f"[ğŸ” ì¶”ë¡ ëœ ê´€ì‹¬ì‚¬] â†’ {predicted_keywords}")
        print(f"[ğŸ“Œ ëˆ„ì  ì •ë³´] ë‚˜ì´: {stored_age}, ì§€ì—­: {stored_region}, ê´€ì‹¬ì‚¬: {stored_interests}" )

        # ëˆ„ì  ì§í›„, ì‚¬ìš©ì ì •ë³´ê°€ ì—¬ì „íˆ ëª¨ë‘ ë¹„ì–´ìˆë‹¤ë©´ ì¶”ì²œ ì°¨ë‹¨
        # ìˆ˜ì •: ìƒˆë¡œ ì¶”ì¶œëœ ê°’ì´ ìˆìœ¼ë©´ ì¶”ì²œ íë¦„ ì§„ì…í•˜ë„ë¡ ì¡°ê±´ ê°•í™”
        age = user_info['age']
        region = user_info['region']
        interests = user_info['interests']

        # ------ â‘  ë²¡í„° ê²€ìƒ‰ & ì„ ë³„ ------
        filters_keywords_only = {
            "categories": {"$in": stored_interests}
        } if stored_interests else None

        docs = []
        if vectordb is not None:
            try:
                # ë§Œì•½ 'ë‹¤ë¥¸ ì •ì±…' ê°™ì€ ì¼ë°˜ ìš”ì²­ì´ë©´ ëˆ„ì  ì •ë³´ë¥¼ ì‚¬ìš©í•´ ì§ˆì˜ ì¬êµ¬ì„±
                search_query = user_input
                if force_more_request:
                    search_query = build_query("ì¶”ì²œ", stored_age, stored_region, stored_interests)
                    # ì¶”ê°€ ìš”ì²­ ì‹œ í•„í„° ì—†ì´ ë” ë§ì€ í›„ë³´ ê²€ìƒ‰
                    raw_docs = vectordb.similarity_search(search_query, k=50)
                else:
                    raw_docs = vectordb.similarity_search(search_query, k=50, filter=filters_keywords_only)
            except Exception:
                search_query = user_input
                if force_more_request:
                    search_query = build_query("ì¶”ì²œ", stored_age, stored_region, stored_interests)
                    raw_docs = vectordb.similarity_search(search_query, k=50)
                else:
                    raw_docs = vectordb.similarity_search(search_query, k=50, filter=filters_keywords_only)

            # âœ… ì§€ì—­Â·ë‚˜ì´Â·ê´€ì‹¬ì‚¬ ê¸°ë°˜ ìŠ¤ì½”ì–´ë§
            user_age_for_score = stored_age if stored_age else 0
            user_region_for_score = stored_region if stored_region else ""
            docs = filter_docs(
                raw_docs,
                user_age_for_score,
                search_query,
                user_region_for_score,
                stored_interests
            )

            docs = docs[:1]  # ìƒìœ„ 1ê±´ë§Œ

        # ------ â‘¡ ì¶œë ¥ ë¡œì§ ------
        if not docs:
            #print("\nBot:\ní˜„ì¬ ì¡°ê±´ì— ë”± ë§ëŠ” ì •ì±…ì´ ë³´ì´ì§€ ì•Šì•„ìš”.")

            # 1) ì£¼ìš”/ì¶”ë¡  ê´€ì‹¬ì‚¬ íŒŒì•…
            main_interest = stored_interests[0] if stored_interests else None
            if not main_interest:
                # ì‚¬ìš©ì ì…ë ¥ì—ì„œ INTEREST_MAPPING í‚¤ì›Œë“œ ìŠ¤ìº”
                for std_i, kws in INTEREST_MAPPING.items():
                    if any(k in user_input for k in kws):
                        main_interest = std_i
                        break

            # 2) ì„¸ë¶€ ê´€ì‹¬ì‚¬ ì§ˆë¬¸ (SUB_INTEREST_MAPPING ì´ìš©)
            clarified = False
            if main_interest:
                sub_map = SUB_INTEREST_MAPPING.get(main_interest)
                if sub_map:
                    sub_options = ", ".join(sub_map.keys())
                    ask = f"{main_interest}ê³¼ ê´€ë ¨í•´ ì–´ë–¤ ì§€ì›ì„ ì°¾ê³  ê³„ì‹ ê°€ìš”? (ì˜ˆ: {sub_options}) : "
                    sub_choice = input(ask).strip()
                    if sub_choice:
                        if sub_choice not in stored_interests:
                            stored_interests.append(sub_choice)
                        refined_query = f"{user_input} {sub_choice}"
                        raw_docs = vectordb.similarity_search(refined_query, k=50)
                        docs = filter_docs(
                            raw_docs,
                            stored_age if stored_age else 0,
                            refined_query,
                            stored_region if stored_region else "",
                            stored_interests
                        )
                        docs = docs[:3]
                        clarified = True

            # 3) ì¼ë°˜ ê´€ì‹¬ì‚¬ ì§ˆë¬¸ (ì„¸ë¶€ ê´€ì‹¬ì‚¬ ì‹¤íŒ¨ ë˜ëŠ” ë§¤í•‘ ì—†ìŒ)
            if not clarified:
                suggestion_list = suggest_remaining_interests(stored_interests)
                prompt_text = f"ì–´ë–¤ ë¶„ì•¼ì˜ ì •ì±…ì„ ì°¾ê³  ê³„ì‹ ê°€ìš”? (ì˜ˆ: {suggestion_list}): "
                generic_choice = input(prompt_text).strip()
                if generic_choice:
                    if generic_choice not in stored_interests:
                        stored_interests.append(generic_choice)
                    start_time = time.time()  # â± reset timer after user input
                    refined_query = f"{user_input} {generic_choice}"
                    raw_docs = vectordb.similarity_search(refined_query, k=50)
                    docs = filter_docs(
                        raw_docs,
                        stored_age if stored_age else 0,
                        refined_query,
                        stored_region if stored_region else "",
                        stored_interests
                    )
                    docs = docs[:3]

            # 4) ìµœì¢… í´ë°±: ì „êµ­ ê³µí†µ ì •ì±…
            if not docs:
                print("ì¡°ê±´ì— ë§ëŠ” ì •ì±…ì´ ì—†ì–´ ì „êµ­ ê³µí†µ ì •ì±… 3ê±´ì„ ëŒ€ì‹  ë³´ì—¬ë“œë¦´ê²Œìš”.\n")
                docs = vectordb.similarity_search("ì²­ë…„ ì •ì±… ì „êµ­ ê³µí†µ", k=3)

        # ğŸ‘‰ ì´ë¯¸ ì œì‹œí•œ ì •ì±…ì€ ì œì™¸í•˜ê³  ìµœëŒ€ 3ê±´ê¹Œì§€ ì¶œë ¥, ì¤‘ë³µ ì œê±° ê°•í™”
        unique_docs = []
        seen_ids = set()              # ì¤‘ë³µ ì œê±°ìš©(í˜„ì¬ íšŒì°¨)
        for d in docs:
            pid = d.metadata.get("policy_id")
            if not pid:
                continue
            if pid in recommended_ids or pid in seen_ids:
                continue  # ì´ë¯¸ ë³´ì—¬ì¤¬ê±°ë‚˜ í˜„ì¬ ë¦¬ìŠ¤íŠ¸ì— ì¤‘ë³µ
            unique_docs.append(d)
            seen_ids.add(pid)
            break  # âœ… ë‹¨ 1ê±´ë§Œ ìˆ˜ì§‘

        # ì¶”ê°€ íƒìƒ‰: ì¤‘ë³µ ì œê±°ë¡œ 1ê±´ì´ ì•ˆ ì±„ì›Œì¡Œì„ ê²½ìš° raw_docsì—ì„œ ë³´ì¶© (seen_idsë„ ì²´í¬)
        if len(unique_docs) < 1:
            # raw_docsê°€ ìˆì„ ë•Œë§Œ
            extra_pool = [rd for rd in raw_docs
                          if rd.metadata.get("policy_id")
                          and rd.metadata.get("policy_id") not in recommended_ids
                          and rd.metadata.get("policy_id") not in seen_ids]
            for rd in extra_pool:
                unique_docs.append(rd)
                seen_ids.add(rd.metadata.get("policy_id"))
                if len(unique_docs) == 1:
                    break
        # ------ â± ì‘ë‹µ ì‹œê°„ ì¸¡ì • ë° ì¶œë ¥ ------
        # â± end_time = time.time()
        # â± elapsed = end_time - start_time
        # â± total_response_time += elapsed
        # â± response_count += 1

        # â± print(f"\nâ± ì‘ë‹µ ì‹œê°„: {elapsed:.2f}ì´ˆ")
        # â± print(f"ğŸ“Š í‰ê·  ì‘ë‹µ ì‹œê°„: {total_response_time / response_count:.2f}ì´ˆ\n")

        if not unique_docs:
            print("Bot:\në” ì´ìƒ ìƒˆë¡œìš´ ì •ì±…ì„ ì°¾ì§€ ëª»í–ˆì–´ìš”. ë‹¤ë¥¸ ì¡°ê±´ì„ ì…ë ¥í•´ ë³´ì‹¤ë˜ìš”?\n")
            continue

        policy_ids = []
        answers    = []
        for doc in unique_docs:
            pid = doc.metadata.get("policy_id", "")
            pname = doc.metadata.get("title", "")
            policy_ids.append(pid)
            answers.append(f"{pname}ì— ì§€ì›í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            recommended_ids.add(pid)          # ê¸°ë¡

        result_obj = {
            "policy_id": policy_ids,
            "answer": answers
        }
        print(json.dumps(result_obj, ensure_ascii=False, indent=2))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
# Helper: Fallback-based document retrieval
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
def retrieve_with_fallback(query, age, region, interests, vectordb, k=5):
    filters = []

    meta = {}
    if region:
        meta["region"] = {"$contains": region}
    if age:
        meta["min_age"] = {"$lte": age}
        meta["max_age"] = {"$gte": age}
    if interests:
        meta["categories"] = {"$in": interests}
    filters.append(meta)

    if "region" in meta:
        f2 = meta.copy()
        del f2["region"]
        filters.append(f2)

    if "min_age" in meta and "max_age" in meta:
        f3 = meta.copy()
        del f3["min_age"]
        del f3["max_age"]
        filters.append(f3)

    if "categories" in meta:
        filters.append({"categories": {"$in": interests}})

    filters.append({})  # no filters

    for f in filters:
        try:
            docs = vectordb.similarity_search(query, filter=f, k=k)
            if docs:
                return docs
        except Exception as e:
            continue

    return []


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
# ğŸ”— FastAPI ì—°ë™ìš© ë‹¨ì¼ ìš”ì²­ ì²˜ë¦¬ í•¨ìˆ˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
from typing import Dict

def _compose_reason(doc: Document, user_info: Dict) -> str:
    """
    ê°„ë‹¨í•œ ì¶”ì²œ ì‚¬ìœ  ë¬¸ìì—´ ìƒì„±
    """
    reasons = []
    age = user_info.get("age")
    region = user_info.get("region")
    interests = user_info.get("interests", [])

    # ë‚˜ì´
    if age is not None:
        min_age = doc.metadata.get("min_age", 0)
        max_age = doc.metadata.get("max_age", 99)
        if min_age <= age <= max_age:
            reasons.append("ë‚˜ì´ ì¡°ê±´ ë¶€í•©")

    # ì§€ì—­
    doc_region = doc.metadata.get("region", "")
    if region:
        if "ì „êµ­" in doc_region or region in doc_region:
            reasons.append("ì§€ì—­ ì¡°ê±´ ë¶€í•©")

    # ê´€ì‹¬ì‚¬
    if interests:
        doc_cats = doc.metadata.get("categories", [])
        if isinstance(doc_cats, str):
            doc_cats = [c.strip() for c in doc_cats.split(",") if c.strip()]
        if set(interests) & set(doc_cats):
            reasons.append("ê´€ì‹¬ì‚¬ ì¡°ê±´ ë¶€í•©")

    return ", ".join(reasons) if reasons else "ì¼ë¶€ ì¡°ê±´ ë¶€í•©"

def generate_policy_response(
    user_id: str,
    user_input: str,
    *,
    vectordb: Chroma = policy_vectordb,
    keyword_vectordb: Chroma = keyword_vectordb,
    category_vectordb: Chroma = category_vectordb,
) -> dict:
    """
    FastAPI ì„œë²„ì—ì„œ í˜¸ì¶œ ê°€ëŠ¥í•œ ë‹¨ì¼ ì§ˆì˜â€‘ì‘ë‹µ í•¨ìˆ˜.
    - user_input: ì‚¬ìš©ìì˜ ìì—°ì–´ ì§ˆë¬¸
    - ë°˜í™˜ í˜•ì‹ì€ ì—…ë¬´ ìš”ì²­ì„œì— ëª…ì‹œëœ JSON êµ¬ì¡°ë¥¼ ë”°ë¥¸ë‹¤.
    """

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
    # ğŸ‘¤ ì„¸ì…˜ ë©”ëª¨ë¦¬ ë¡œë“œ & ë¨¸ì§€
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
    session               = SESSION_STORE[user_id]          # dict with 'user_info', 'recommended_ids'
    prev_info             = session["user_info"] or {}
    prev_recommended_ids  = session["recommended_ids"]

    # 1) ìƒˆ ì…ë ¥ì—ì„œ ì •ë³´ ì¶”ì¶œ
    current_info = extract_user_info(user_input)

    # 2) ì´ì „ ì •ë³´ì™€ ë³‘í•© (ìƒˆ ê°’ì´ ìˆìœ¼ë©´ ë®ì–´ì”€)
    merged_info = prev_info.copy()
    for k, v in current_info.items():
        if v:  # ê°’ì´ None/ë¹ˆ ë¦¬ìŠ¤íŠ¸/ë¹ˆ ë¬¸ìì—´ì´ ì•„ë‹ˆë©´
            merged_info[k] = v

    # 3) ì´í›„ ë¡œì§ì€ merged_info ì‚¬ìš©
    user_info = merged_info
    age       = user_info.get("age")
    region    = user_info.get("region")
    interests = list(user_info.get("interests", []))  # copy

    # 2) í•„ìˆ˜ ì •ë³´ í™•ì¸ -----------------------------------------------
    # ğŸ‘‰ ëˆ„ì  ì •ë³´ë¥¼ ì„¸ì…˜ì— ì¦‰ì‹œ ì €ì¥í•´ ë¶€ë¶„ ì…ë ¥ë„ ê¸°ì–µ
    session["user_info"] = user_info

    # ëˆ„ë½ í•­ëª© ì‹ë³„
    missing = []
    if age is None:
        missing.append("age")
    if region is None:
        missing.append("region")
    if not interests:
        missing.append("interests")

    if missing:
        label_map = {"age": "ë‚˜ì´", "region": "ì§€ì—­", "interests": "ê´€ì‹¬ì‚¬"}
        missing_kor = [label_map[m] for m in missing]
        prompt_text = f"{', '.join(missing_kor)}ë¥¼ ì•Œë ¤ì£¼ì‹œë©´ ë§ì¶¤í˜• ì •ì±…ì„ ì¶”ì²œí•´ë“œë¦´ê²Œìš”."
        return {
            "message": prompt_text,
            "missing_info": missing,
        }

    # 3) (ì„ íƒ) ê´€ì‹¬ì‚¬ ë³´ê°• -------------------------------------------
    #    ë²¡í„° DBë¥¼ ì´ìš©í•´ ì¶”ê°€ ê´€ì‹¬ì‚¬ë¥¼ ì˜ˆì¸¡í•˜ê³ , ê¸°ì¡´ ê´€ì‹¬ì‚¬ì— ë³‘í•©
    predicted_interests = []
    embedding = None

    if keyword_vectordb:
        embedding = OpenAIEmbeddings()
        qvec = embedding.embed_query(user_input)
        docs = keyword_vectordb.similarity_search_by_vector(qvec, k=3)
        predicted_interests.extend([d.page_content for d in docs])

    if not predicted_interests and category_vectordb:
        if embedding is None:
            embedding = OpenAIEmbeddings()
        qvec = embedding.embed_query(user_input)
        docs = category_vectordb.similarity_search_by_vector(qvec, k=2)
        predicted_interests.extend([d.page_content for d in docs])

    # INTEREST_MAPPING ê¸°ë°˜ í‘œì¤€í™”
    std_preds = []
    for kw in predicted_interests:
        if kw in INTEREST_MAPPING and kw not in std_preds:
            std_preds.append(kw)
            continue
        for std_i, kws in INTEREST_MAPPING.items():
            if kw in kws and std_i not in std_preds:
                std_preds.append(std_i)
                break

    # ë³‘í•©
    for p in std_preds:
        if p not in interests:
            interests.append(p)

    # 4) ë²¡í„° ê²€ìƒ‰ + í•„í„°ë§ -------------------------------------------
    search_query = build_query(user_input, age, region, interests)
    try:
        raw_docs = vectordb.similarity_search(search_query, k=50)
    except Exception:
        # ê²€ìƒ‰ ì˜¤ë¥˜ ì‹œ ìµœì†Œí•œì˜ ì§ˆì˜ë¡œ ì¬ì‹œë„
        raw_docs = vectordb.similarity_search(user_input, k=50)

    docs = filter_docs(raw_docs, age, search_query, region, interests)

    # ğŸ” ì´ì „ì— ì¶”ì²œí–ˆë˜ ì •ì±…ì€ ì œì™¸ + ì¤‘ë³µ ì‘ë‹µ ì°¨ë‹¨
    seen_ids = set()
    filtered_docs = []
    for d in docs:
        pid = d.metadata.get("policy_id")
        if not pid or pid in prev_recommended_ids or pid in seen_ids:
            continue
        filtered_docs.append(d)
        seen_ids.add(pid)
        if len(filtered_docs) == 3:
            break
    docs = filtered_docs

    # 5) ê²°ê³¼ê°€ ì—†ì„ ë•Œ í´ë°± ------------------------------------------
    if not docs:
        fallback_docs = vectordb.similarity_search("ì²­ë…„ ì •ì±… ì „êµ­ ê³µí†µ", k=3)
        policies = []
        for d in fallback_docs:
            policies.append({
                "policy_id": d.metadata.get("policy_id", ""),
                "title":     d.metadata.get("title", ""),
                "summary":   d.metadata.get("summary", "") or d.page_content[:120],
            })
        # ğŸ‘‰ ì„¸ì…˜ ì—…ë°ì´íŠ¸ (fallbackë„ ê¸°ë¡)
        session["recommended_ids"].update([p["policy_id"] for p in policies])
        session["user_info"] = user_info
        return {
            "message": "ì¡°ê±´ì— ë§ëŠ” ì •ì±…ì„ ì°¾ì§€ ëª»í–ˆì–´ìš”. ì „êµ­ ê³µí†µ ì •ì±…ì„ ë³´ì—¬ë“œë¦´ê²Œìš”.",
            "fallback_policies": policies,
            "user_info": user_info,
        }

    # 6) ì •ìƒ ì¶”ì²œ -----------------------------------------------------
    policies = []
    for d in docs:
        apply_url = d.metadata.get("apply_url", "")
        if not apply_url:
            # ğŸ” í˜ì´ì§€ ë³¸ë¬¸ì´ë‚˜ ìš”ì•½ì—ì„œ URL íŒ¨í„´ ì¶”ì¶œ
            m = re.search(r"https?://[^\s)]+", d.page_content)
            if not m:
                m = re.search(r"https?://[^\s)]+", d.metadata.get("summary", ""))
            if m:
                apply_url = m.group(0)
        policies.append({
            "policy_id": d.metadata.get("policy_id", ""),
            "title":     d.metadata.get("title", ""),
            "summary":   d.metadata.get("summary", "") or d.page_content[:120],
            "apply_url": apply_url,
            "reason":    _compose_reason(d, user_info),
        })

    # ğŸ‘‰ ì„¸ì…˜ ì—…ë°ì´íŠ¸
    session["recommended_ids"].update([p["policy_id"] for p in policies])
    session["user_info"] = user_info

    # ë³‘í•©
    user_info["interests"] = interests

    return {
        "message": "ì¶”ì²œ ì •ì±…ì„ ì•ˆë‚´ë“œë¦½ë‹ˆë‹¤.",
        "policies": policies,
        "user_info": user_info,
    }